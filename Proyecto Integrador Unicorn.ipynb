{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83be5f93-a06f-44d2-91a5-373ec21531d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\anaconda3\\envs\\unicornenv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3d00c6-141f-4fe9-8406-a90cdc0e0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a63e85-cf80-40ff-8b3c-c6bdd33d3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando ruta...\n",
      "Ruta completa: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv\n",
      "Existe la carpeta? True\n",
      "Existe el archivo? True\n",
      "Archivo leído exitosamente.\n",
      "Primeras 5 filas:\n",
      "  State Name county Name  State Code  County Code        Date  AQI  Category  \\\n",
      "0    Alabama     Baldwin           1            3  2015-01-03   38      Good   \n",
      "1    Alabama     Baldwin           1            3  2015-01-06   55  Moderate   \n",
      "2    Alabama     Baldwin           1            3  2015-01-09   60  Moderate   \n",
      "3    Alabama     Baldwin           1            3  2015-01-12   52  Moderate   \n",
      "4    Alabama     Baldwin           1            3  2015-01-15   35      Good   \n",
      "\n",
      "  Defining Parameter Defining Site  Number of Sites Reporting  \n",
      "0              PM2.5   01-003-0010                          1  \n",
      "1              PM2.5   01-003-0010                          1  \n",
      "2              PM2.5   01-003-0010                          1  \n",
      "3              PM2.5   01-003-0010                          1  \n",
      "4              PM2.5   01-003-0010                          1  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ruta que estamos intentando usar\n",
    "file_path = \"C:\\\\Users\\\\mjcd1\\\\Desktop\\\\CURSOS\\\\Python Unicorn\\\\Proyecto Integrador Final Calidad del Aire USA\\\\Air Quality Data Project\\\\Daily AQI by County\\\\daily_aqi_by_county_2015\\\\daily_aqi_by_county_2015.csv\"\n",
    "\n",
    "\n",
    "# Mostrar detalles de la ruta\n",
    "print(\"Verificando ruta...\")\n",
    "print(f\"Ruta completa: {file_path}\")\n",
    "print(f\"Existe la carpeta? {os.path.exists(os.path.dirname(file_path))}\")\n",
    "print(f\"Existe el archivo? {os.path.isfile(file_path)}\")\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    import pandas as pd\n",
    "    df_aqi_county_2015 = pd.read_csv(file_path, encoding='utf-8')\n",
    "    print(\"Archivo leído exitosamente.\")\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(df_aqi_county_2015.head())\n",
    "else:\n",
    "    print(\"El archivo no se encuentra. Por favor, verifica la ruta o el nombre del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d604b5c-14f1-4e4c-afbf-6ba38b2a0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv → CO_42101\n",
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2015\\daily_aqi_by_cbsa_2015.csv → Daily AQI by CBSA\n",
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv → Daily AQI by County\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv → NO2_42602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv → Ozone_44201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv → SO2_42401\n",
      "\n",
      "✅ 6 DataFrames cargados (uno por código)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ruta_base = r\"C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\"\n",
    "dataframes_por_codigo = {}\n",
    "\n",
    "for carpeta_codigo in os.listdir(ruta_base):\n",
    "    ruta_codigo = os.path.join(ruta_base, carpeta_codigo)\n",
    "    \n",
    "    if os.path.isdir(ruta_codigo):\n",
    "        # Buscamos la subcarpeta del año 2015\n",
    "        subcarpeta_2015 = [d for d in os.listdir(ruta_codigo) if os.path.isdir(os.path.join(ruta_codigo, d)) and '2015' in d]\n",
    "        \n",
    "        if subcarpeta_2015:\n",
    "            ruta_2015 = os.path.join(ruta_codigo, subcarpeta_2015[0])\n",
    "            \n",
    "            # Buscamos el CSV dentro de esa subcarpeta\n",
    "            archivos_csv = [f for f in os.listdir(ruta_2015) if f.endswith('.csv')]\n",
    "            \n",
    "            if archivos_csv:\n",
    "                ruta_csv = os.path.join(ruta_2015, archivos_csv[0])\n",
    "                \n",
    "                df = pd.read_csv(ruta_csv)\n",
    "                dataframes_por_codigo[carpeta_codigo] = df\n",
    "                \n",
    "                print(f\"📥 Cargado: {ruta_csv} → {carpeta_codigo}\")\n",
    "\n",
    "print(f\"\\n✅ {len(dataframes_por_codigo)} DataFrames cargados (uno por código)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f11da9-e1ca-44cf-b72b-4bd05fbf0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CO_42101', 'Daily AQI by CBSA', 'Daily AQI by County', 'NO2_42602', 'Ozone_44201', 'SO2_42401'])\n"
     ]
    }
   ],
   "source": [
    "print(dataframes_por_codigo.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7b4d52-6f16-474a-95a7-5405f1165081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42101    2  33.553056   \n",
      "1           1           73        23           42101    2  33.553056   \n",
      "2           1           73        23           42101    2  33.553056   \n",
      "3           1           73        23           42101    2  33.553056   \n",
      "4           1           73        23           42101    2  33.553056   \n",
      "\n",
      "   Longitude  Datum   Parameter Name  Date Local  ...   Units of Measure  \\\n",
      "0    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "1    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "2    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "3    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "4    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "\n",
      "    MDL Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0  0.04         NaN        NaN         FRM          554   \n",
      "1  0.04         NaN        NaN         FRM          554   \n",
      "2  0.04         NaN        NaN         FRM          554   \n",
      "3  0.04         NaN        NaN         FRM          554   \n",
      "4  0.04         NaN        NaN         FRM          554   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "1  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "2  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "3  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "4  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['CO_42101']\n",
    "print(df.head())  # Muestra las primeras 5 filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d13c2c-8bc4-47d4-9599-cfdfe0f7fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CBSA  CBSA Code        Date  AQI Category Defining Parameter  \\\n",
      "0  Aberdeen, SD      10100  2015-01-03   35     Good              PM2.5   \n",
      "1  Aberdeen, SD      10100  2015-01-06   38     Good              PM2.5   \n",
      "2  Aberdeen, SD      10100  2015-01-09   33     Good              PM2.5   \n",
      "3  Aberdeen, SD      10100  2015-01-12   35     Good              PM2.5   \n",
      "4  Aberdeen, SD      10100  2015-01-15   28     Good              PM2.5   \n",
      "\n",
      "  Defining Site  Number of Sites Reporting  \n",
      "0   46-013-0003                          1  \n",
      "1   46-013-0003                          1  \n",
      "2   46-013-0003                          1  \n",
      "3   46-013-0003                          1  \n",
      "4   46-013-0003                          1  \n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Daily AQI by CBSA']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476897d8-39a9-4a02-ac3a-a9b3728e26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State Name county Name  State Code  County Code        Date  AQI  Category  \\\n",
      "0    Alabama     Baldwin           1            3  2015-01-03   38      Good   \n",
      "1    Alabama     Baldwin           1            3  2015-01-06   55  Moderate   \n",
      "2    Alabama     Baldwin           1            3  2015-01-09   60  Moderate   \n",
      "3    Alabama     Baldwin           1            3  2015-01-12   52  Moderate   \n",
      "4    Alabama     Baldwin           1            3  2015-01-15   35      Good   \n",
      "\n",
      "  Defining Parameter Defining Site  Number of Sites Reporting  \n",
      "0              PM2.5   01-003-0010                          1  \n",
      "1              PM2.5   01-003-0010                          1  \n",
      "2              PM2.5   01-003-0010                          1  \n",
      "3              PM2.5   01-003-0010                          1  \n",
      "4              PM2.5   01-003-0010                          1  \n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Daily AQI by County']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249e50c7-9337-4cc7-9989-a0d9e7e90f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42602    1  33.553056   \n",
      "1           1           73        23           42602    1  33.553056   \n",
      "2           1           73        23           42602    1  33.553056   \n",
      "3           1           73        23           42602    1  33.553056   \n",
      "4           1           73        23           42602    1  33.553056   \n",
      "\n",
      "   Longitude  Datum          Parameter Name  Date Local  ...  \\\n",
      "0    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "1    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "2    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "3    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "4    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "\n",
      "    Units of Measure  MDL Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "1  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "2  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "3  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "4  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "1  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "2  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "3  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "4  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['NO2_42602']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffc88e0-1449-4909-9de5-a0a6d1bcc8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1            3        10           44201    1  30.497478   \n",
      "1           1            3        10           44201    1  30.497478   \n",
      "2           1            3        10           44201    1  30.497478   \n",
      "3           1            3        10           44201    1  30.497478   \n",
      "4           1            3        10           44201    1  30.497478   \n",
      "\n",
      "   Longitude  Datum Parameter Name  Date Local  ...   Units of Measure    MDL  \\\n",
      "0 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "1 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "2 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "3 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "4 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "\n",
      "  Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0         NaN        NaN         FEM           47   \n",
      "1         NaN        NaN         FEM           47   \n",
      "2         NaN        NaN         FEM           47   \n",
      "3         NaN        NaN         FEM           47   \n",
      "4         NaN        NaN         FEM           47   \n",
      "\n",
      "                   Method Name State Name County Name  Date of Last Change  \n",
      "0  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "1  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "2  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "3  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "4  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Ozone_44201']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366d3a48-fe08-4a68-afaf-961f5b11ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42401    2  33.553056   \n",
      "1           1           73        23           42401    2  33.553056   \n",
      "2           1           73        23           42401    2  33.553056   \n",
      "3           1           73        23           42401    2  33.553056   \n",
      "4           1           73        23           42401    2  33.553056   \n",
      "\n",
      "   Longitude  Datum  Parameter Name  Date Local  ...   Units of Measure  MDL  \\\n",
      "0    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "1    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "2    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "3    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "4    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "\n",
      "  Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0         NaN        NaN         FEM          560   \n",
      "1         NaN        NaN         FEM          560   \n",
      "2         NaN        NaN         FEM          560   \n",
      "3         NaN        NaN         FEM          560   \n",
      "4         NaN        NaN         FEM          560   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "1  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "2  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "3  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "4  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['SO2_42401']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b389620b-5615-4fb5-a85b-f8d9d0abfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            9002938\n",
       "Qualifier              8775861\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Ozone_44201'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9878a2e2-f39e-40cb-a926-9d81c410f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            3722618\n",
       "Qualifier              3580007\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['SO2_42401'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f62a737-d351-4d2c-a932-c1febdb5b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            3549185\n",
       "Qualifier              3436967\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['NO2_42602'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "075efd66-9a66-4142-ae42-d7a724b2c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Name                   0\n",
       "county Name                  0\n",
       "State Code                   0\n",
       "County Code                  0\n",
       "Date                         0\n",
       "AQI                          0\n",
       "Category                     0\n",
       "Defining Parameter           0\n",
       "Defining Site                0\n",
       "Number of Sites Reporting    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Daily AQI by County'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681f497c-7c04-45c9-bdcc-309e833908f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBSA                         0\n",
       "CBSA Code                    0\n",
       "Date                         0\n",
       "AQI                          0\n",
       "Category                     0\n",
       "Defining Parameter           0\n",
       "Defining Site                0\n",
       "Number of Sites Reporting    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Daily AQI by CBSA'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c114d26-a9fa-427e-b623-e40b66df2dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            2450576\n",
       "Qualifier              2382483\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['CO_42101'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd80f40-7bf3-494c-9c38-47b0fef76144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State Name            County Name  location_id\n",
      "0          Alabama              Jefferson            1\n",
      "1           Alaska             Anchorage             2\n",
      "2           Alaska  Fairbanks North Star             3\n",
      "3          Arizona               Maricopa            4\n",
      "4          Arizona                   Pima            5\n",
      "..             ...                    ...          ...\n",
      "900       Virginia                    NaN          901\n",
      "901     Washington                    NaN          902\n",
      "902  West Virginia                    NaN          903\n",
      "903      Wisconsin                    NaN          904\n",
      "904        Wyoming                    NaN          905\n",
      "\n",
      "[905 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lista de DataFrames que contienen columnas de ubicación\n",
    "dfs = [\n",
    "    dataframes_por_codigo['CO_42101'],\n",
    "    dataframes_por_codigo['NO2_42602'],\n",
    "    dataframes_por_codigo['SO2_42401'],\n",
    "    dataframes_por_codigo['Ozone_44201'],\n",
    "    dataframes_por_codigo['Daily AQI by County']\n",
    "]\n",
    "\n",
    "# Unir todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extraer combinaciones únicas de estado y condado\n",
    "ubicaciones_unicas = df_combinado[['State Name', 'County Name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Asignar un ID artificial\n",
    "ubicaciones_unicas['location_id'] = ubicaciones_unicas.index + 1\n",
    "\n",
    "# Mostrar resultado\n",
    "print(ubicaciones_unicas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac1cdd-9d89-4150-a2ee-7aa8d3ef7460",
   "metadata": {},
   "source": [
    "## **🐍 Función en Python para exportar sentencias SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8641079-4999-4e4c-9256-eb32df2017ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando DataFrames de CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:13: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_co = pd.read_csv(os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de NO2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:20: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_no2 = pd.read_csv(os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de SO2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:27: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_so2 = pd.read_csv(os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO2 cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de Ozono...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:34: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ozone = pd.read_csv(os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ozono cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de Daily AQI by County (para información de condados sin Lat/Lon)...\n",
      "Daily AQI by County cargado. Columnas: ['State Name', 'county Name', 'State Code', 'County Code', 'Date', 'AQI', 'Category', 'Defining Parameter', 'Defining Site', 'Number of Sites Reporting']\n",
      "\n",
      "Iniciando la exportación SQL para la tabla 'location'...\n",
      "INFO: Excluyendo 'Daily AQI by County' de la concatenación para 'location' ya que no contiene todas las columnas de ubicación (Latitude, Longitude, Datum).\n",
      "\n",
      "--- DEPURATION: DataFrame 'ubicaciones' antes de exportar ---\n",
      "Columnas finales del DataFrame 'ubicaciones': ['location_id', 'State Name', 'State Code', 'County Name', 'County Code', 'Latitude', 'Longitude', 'Datum']\n",
      "Primeras 10 filas del DataFrame 'ubicaciones':\n",
      "   location_id State Name  State Code            County Name  County Code  \\\n",
      "0            1    Alabama           1              Jefferson           73   \n",
      "1            2    Alabama           1              Jefferson           73   \n",
      "2            3    Alabama           1              Jefferson           73   \n",
      "3            4    Alabama           1              Jefferson           73   \n",
      "4            5     Alaska           2             Anchorage            20   \n",
      "5            6     Alaska           2  Fairbanks North Star            90   \n",
      "6            7    Arizona           4               Maricopa           13   \n",
      "7            8    Arizona           4               Maricopa           13   \n",
      "8            9    Arizona           4               Maricopa           13   \n",
      "9           10    Arizona           4               Maricopa           13   \n",
      "\n",
      "    Latitude   Longitude  Datum  \n",
      "0  33.553056  -86.815000  WGS84  \n",
      "1  33.485556  -86.915000  WGS84  \n",
      "2  33.521427  -86.844112  WGS84  \n",
      "3  33.565278  -86.796389  WGS84  \n",
      "4  61.205861 -149.824602  WGS84  \n",
      "5  64.845690 -147.727413  WGS84  \n",
      "6  33.483780 -112.142560  NAD83  \n",
      "7  33.410180 -111.865360  NAD83  \n",
      "8  33.560310 -112.066190  NAD83  \n",
      "9  33.574530 -112.191930  NAD83  \n",
      "Número de filas en 'ubicaciones': 1590\n",
      "Tipos de datos de las columnas en 'ubicaciones':\n",
      "location_id      int64\n",
      "State Name      object\n",
      "State Code       int64\n",
      "County Name     object\n",
      "County Code      int64\n",
      "Latitude       float64\n",
      "Longitude      float64\n",
      "Datum           object\n",
      "dtype: object\n",
      "Conteo de nulos en columnas clave de 'ubicaciones':\n",
      "State Code     0\n",
      "County Code    0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "Datum          0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Archivo 'insert_locations.sql' generado con 1590 ubicaciones completas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' #\n",
    "\n",
    "# --- 2. Cargar los DataFrames necesarios para la función ---\n",
    "dataframes_por_codigo = {}\n",
    "\n",
    "try:\n",
    "    print(\"Cargando DataFrames de CO...\")\n",
    "    # Eliminado nrows=100 para cargar todos los datos\n",
    "    df_co = pd.read_csv(os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv')) #\n",
    "    df_co.columns = df_co.columns.str.strip() # Limpiar nombres de columna\n",
    "    dataframes_por_codigo['CO_42101'] = df_co\n",
    "    print(f\"CO cargado. Columnas: {df_co.columns.tolist()}\") #\n",
    "\n",
    "    print(\"Cargando DataFrames de NO2...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_no2 = pd.read_csv(os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv')) #\n",
    "    df_no2.columns = df_no2.columns.str.strip()\n",
    "    dataframes_por_codigo['NO2_42602'] = df_no2\n",
    "    print(f\"NO2 cargado. Columnas: {df_no2.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de SO2...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_so2 = pd.read_csv(os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv')) #\n",
    "    df_so2.columns = df_so2.columns.str.strip()\n",
    "    dataframes_por_codigo['SO2_42401'] = df_so2\n",
    "    print(f\"SO2 cargado. Columnas: {df_so2.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de Ozono...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_ozone = pd.read_csv(os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv')) #\n",
    "    df_ozone.columns = df_ozone.columns.str.strip()\n",
    "    dataframes_por_codigo['Ozone_44201'] = df_ozone\n",
    "    print(f\"Ozono cargado. Columnas: {df_ozone.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de Daily AQI by County (para información de condados sin Lat/Lon)...\")\n",
    "    # Este DF se carga pero NO se usará para la tabla 'location' si 'location' requiere Lat/Lon/Datum.\n",
    "    # Si lo necesitas para otra dimensión (ej. dim_geografia), asegúrate de usarlo allí.\n",
    "    # Eliminado nrows=100\n",
    "    df_daily_county = pd.read_csv(os.path.join(base_project_path, 'Daily AQI by County', 'daily_aqi_by_county_2015', 'daily_aqi_by_county_2015.csv')) #\n",
    "    df_daily_county.columns = df_daily_county.columns.str.strip()\n",
    "    dataframes_por_codigo['Daily AQI by County'] = df_daily_county\n",
    "    print(f\"Daily AQI by County cargado. Columnas: {df_daily_county.columns.tolist()}\") #\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: No se encontró el archivo: {e}\")\n",
    "    print(\"Por favor, verifica que la ruta base y las rutas de los archivos CSV sean correctas.\")\n",
    "    raise SystemExit # Detiene la ejecución si no se encuentran los archivos\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al cargar los Dataframes: {e}\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# --- 3. Definición de la función exportar_sql_completo ---\n",
    "def exportar_sql_completo(dataframes_por_codigo, nombre_archivo=\"insert_locations.sql\"):\n",
    "    print(\"\\nIniciando la exportación SQL para la tabla 'location'...\")\n",
    "    \n",
    "    # Definimos todas las columnas que esperamos tener en el DataFrame final para la tabla `location`\n",
    "    expected_location_cols = [\n",
    "        'State Name', 'State Code', 'County Name', 'County Code',\n",
    "        'Latitude', 'Longitude', 'Datum'\n",
    "    ]\n",
    "\n",
    "    dfs_to_concat = []\n",
    "    # ITERACIÓN CLAVE: EXCLUIMOS 'Daily AQI by County' de esta concatenación\n",
    "    # ya que no tiene Latitude, Longitude, Datum\n",
    "    for key, df in dataframes_por_codigo.items():\n",
    "        if key != 'Daily AQI by County': # EXCLUSIÓN AQUI\n",
    "            # Asegurarse de que cada DF tenga las columnas necesarias. Si no las tiene, añadir NaN.\n",
    "            for col in expected_location_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            # Seleccionamos solo las columnas relevantes antes de concatenar\n",
    "            dfs_to_concat.append(df[expected_location_cols])\n",
    "        else:\n",
    "            print(f\"INFO: Excluyendo '{key}' de la concatenación para 'location' ya que no contiene todas las columnas de ubicación (Latitude, Longitude, Datum).\")\n",
    "\n",
    "\n",
    "    # Concatenar DataFrames relevantes (ahora sin Daily AQI by County)\n",
    "    if not dfs_to_concat:\n",
    "        print(\"ADVERTENCIA: No hay DataFrames con datos de Latitud/Longitud para generar la tabla 'location'.\")\n",
    "        return # Salir de la función si no hay DFs para concatenar\n",
    "\n",
    "    df_completo = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "\n",
    "    # Filtrar y limpiar\n",
    "    # dropna en las columnas que definen una ubicación única y son obligatorias\n",
    "    ubicaciones = (\n",
    "        df_completo\n",
    "        .dropna(subset=['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum'])\n",
    "        .drop_duplicates(subset=['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum'])\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    ubicaciones['location_id'] = ubicaciones.index + 1\n",
    "\n",
    "    # Reordenar las columnas para que 'location_id' esté al principio\n",
    "    ubicaciones = ubicaciones[['location_id'] + expected_location_cols]\n",
    "\n",
    "\n",
    "    # --- Depuración adicional para verificar las columnas y datos ---\n",
    "    print(f\"\\n--- DEPURATION: DataFrame 'ubicaciones' antes de exportar ---\")\n",
    "    print(f\"Columnas finales del DataFrame 'ubicaciones': {ubicaciones.columns.tolist()}\")\n",
    "    print(f\"Primeras 10 filas del DataFrame 'ubicaciones':\\n{ubicaciones.head(10)}\")\n",
    "    print(f\"Número de filas en 'ubicaciones': {len(ubicaciones)}\")\n",
    "    print(f\"Tipos de datos de las columnas en 'ubicaciones':\\n{ubicaciones.dtypes}\")\n",
    "    \n",
    "    # Contar nulos para columnas clave\n",
    "    print(f\"Conteo de nulos en columnas clave de 'ubicaciones':\\n{ubicaciones[['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum']].isnull().sum()}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    # Función para limpiar texto y manejar valores NaN para SQL\n",
    "    def limpiar(valor):\n",
    "        if pd.isna(valor):\n",
    "            return 'NULL'\n",
    "        return str(valor).strip().replace(\"'\", \"''\")\n",
    "\n",
    "    # Exportar archivo SQL\n",
    "    with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INSERT INTO location (location_id, state_name, state_code, county_name, county_code, latitude, longitude, datum) VALUES\\n\")\n",
    "        \n",
    "        for i, fila in ubicaciones.iterrows():\n",
    "            valores_finales = [\n",
    "                fila['location_id'],\n",
    "                f\"'{limpiar(fila['State Name'])}'\" if limpiar(fila['State Name']) != 'NULL' else 'NULL',\n",
    "                f\"{int(fila['State Code'])}\" if pd.notna(fila['State Code']) else 'NULL',\n",
    "                f\"'{limpiar(fila['County Name'])}'\" if limpiar(fila['County Name']) != 'NULL' else 'NULL',\n",
    "                f\"{int(fila['County Code'])}\" if pd.notna(fila['County Code']) else 'NULL',\n",
    "                f\"{float(fila['Latitude'])}\",\n",
    "                f\"{float(fila['Longitude'])}\",\n",
    "                f\"'{limpiar(fila['Datum'])}'\" if limpiar(fila['Datum']) != 'NULL' else 'NULL'\n",
    "            ]\n",
    "\n",
    "            linea = f\"({', '.join(map(str, valores_finales))})\"\n",
    "            f.write(linea + (\",\\n\" if i < len(ubicaciones) - 1 else \";\\n\"))\n",
    "\n",
    "    print(f\"✅ Archivo '{nombre_archivo}' generado con {len(ubicaciones)} ubicaciones completas.\")\n",
    "\n",
    "# --- 4. Llamada a la función para generar el archivo SQL ---\n",
    "exportar_sql_completo(dataframes_por_codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d7f568-99cf-4487-94d1-2e7131b1b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando datos para dim_aqi_categoria...\n",
      "Primeras 5 filas de dim_aqi_categoria_df:\n",
      "                 nombre_categoria  min_aqi_rango  max_aqi_rango color_asociado\n",
      "0                            Good              0             50          Green\n",
      "1                        Moderate             51            100         Yellow\n",
      "2  Unhealthy for Sensitive Groups            101            150         Orange\n",
      "3                       Unhealthy            151            200            Red\n",
      "4                  Very Unhealthy            201            300         Purple\n",
      "Número total de categorías AQI a cargar: 6\n",
      "✅ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_dim_aqi_categoria.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta de salida ---\n",
    "# Asegúrate de que esta ruta sea donde quieres que se guarde el archivo SQL\n",
    "output_sql_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Ajusta si es necesario\n",
    "\n",
    "# --- 2. Definir los datos de las categorías AQI ---\n",
    "aqi_categorias_data = [\n",
    "    {\"nombre_categoria\": \"Good\", \"min_aqi_rango\": 0, \"max_aqi_rango\": 50, \"color_asociado\": \"Green\"},\n",
    "    {\"nombre_categoria\": \"Moderate\", \"min_aqi_rango\": 51, \"max_aqi_rango\": 100, \"color_asociado\": \"Yellow\"},\n",
    "    {\"nombre_categoria\": \"Unhealthy for Sensitive Groups\", \"min_aqi_rango\": 101, \"max_aqi_rango\": 150, \"color_asociado\": \"Orange\"},\n",
    "    {\"nombre_categoria\": \"Unhealthy\", \"min_aqi_rango\": 151, \"max_aqi_rango\": 200, \"color_asociado\": \"Red\"},\n",
    "    {\"nombre_categoria\": \"Very Unhealthy\", \"min_aqi_rango\": 201, \"max_aqi_rango\": 300, \"color_asociado\": \"Purple\"},\n",
    "    {\"nombre_categoria\": \"Hazardous\", \"min_aqi_rango\": 301, \"max_aqi_rango\": 500, \"color_asociado\": \"Maroon\"}\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Pandas\n",
    "df_aqi_categorias = pd.DataFrame(aqi_categorias_data)\n",
    "\n",
    "# No necesitamos sk_aqi_categoria ya que es AUTO_INCREMENT en la BBDD\n",
    "\n",
    "print(\"Generando datos para dim_aqi_categoria...\")\n",
    "print(f\"Primeras 5 filas de dim_aqi_categoria_df:\\n{df_aqi_categorias.head()}\")\n",
    "print(f\"Número total de categorías AQI a cargar: {len(df_aqi_categorias)}\")\n",
    "\n",
    "# --- 3. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_dim_aqi_categoria.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO dim_aqi_categoria (nombre_categoria, min_aqi_rango, max_aqi_rango, color_asociado) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_aqi_categorias.iterrows():\n",
    "        # Escapar comillas simples en los nombres si fuera necesario (aunque aquí no lo son)\n",
    "        nombre_categoria = fila['nombre_categoria'].replace(\"'\", \"''\")\n",
    "        color_asociado = fila['color_asociado'].replace(\"'\", \"''\")\n",
    "\n",
    "        linea = (\n",
    "            f\"('{nombre_categoria}', \"\n",
    "            f\"{fila['min_aqi_rango']}, \"\n",
    "            f\"{fila['max_aqi_rango']}, \"\n",
    "            f\"'{color_asociado}')\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_categorias) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f213982-6675-4c0a-a0f6-799fc5a445a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo datos de parámetros de los DataFrames...\n",
      "✅ Cargado y extraído parámetros de: CO_42101\n",
      "✅ Cargado y extraído parámetros de: NO2_42602\n",
      "✅ Cargado y extraído parámetros de: SO2_42401\n",
      "✅ Cargado y extraído parámetros de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_unique_parameters' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_unique_parameters': ['Parameter Code', 'Parameter Name']\n",
      "Primeras 5 filas del DataFrame 'df_unique_parameters':\n",
      "   Parameter Code          Parameter Name\n",
      "0           42101         Carbon monoxide\n",
      "1           42602  Nitrogen dioxide (NO2)\n",
      "2           42401          Sulfur dioxide\n",
      "3           44201                   Ozone\n",
      "Número total de parámetros únicos a cargar: 4\n",
      "Tipos de datos de las columnas en 'df_unique_parameters':\n",
      "Parameter Code     int64\n",
      "Parameter Name    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_parameter.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto (donde están tus carpetas de datos) ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path # Guardar el SQL en la misma carpeta base\n",
    "\n",
    "# --- 3. Cargar los DataFrames de contaminantes ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_parameters = []\n",
    "\n",
    "print(\"Extrayendo datos de parámetros de los DataFrames...\")\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=['Parameter Code', 'Parameter Name'])\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Seleccionar solo las columnas de interés y añadir a la lista\n",
    "        all_parameters.append(df[['Parameter Code', 'Parameter Name']])\n",
    "        print(f\"✅ Cargado y extraído parámetros de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ADVERTENCIA: Archivo no encontrado para {key} en {path}. Será omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_parameters:\n",
    "    print(\"No se pudieron cargar datos de parámetros de ningún archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar todos los DataFrames de parámetros\n",
    "df_parameters = pd.concat(all_parameters, ignore_index=True)\n",
    "\n",
    "# Obtener parámetros únicos\n",
    "df_unique_parameters = df_parameters.drop_duplicates(subset=['Parameter Code']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_unique_parameters' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_unique_parameters': {df_unique_parameters.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_unique_parameters':\\n{df_unique_parameters.head()}\")\n",
    "print(f\"Número total de parámetros únicos a cargar: {len(df_unique_parameters)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_unique_parameters':\\n{df_unique_parameters.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_parameter.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO parameter (parameter_code, parameter_name) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_unique_parameters.iterrows():\n",
    "        # Limpiar y escapar nombres para SQL\n",
    "        parameter_name = str(fila['Parameter Name']).strip().replace(\"'\", \"''\") if pd.notna(fila['Parameter Name']) else 'NULL'\n",
    "        parameter_code = int(fila['Parameter Code']) if pd.notna(fila['Parameter Code']) else 'NULL'\n",
    "\n",
    "        # Asegurarse de que 'NULL' no tenga comillas\n",
    "        name_for_sql = f\"'{parameter_name}'\" if parameter_name != 'NULL' else 'NULL'\n",
    "        code_for_sql = str(parameter_code) if parameter_code != 'NULL' else 'NULL'\n",
    "\n",
    "        linea = f\"({code_for_sql}, {name_for_sql})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_unique_parameters) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4909a74b-fbac-4317-84d7-6993bd97b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo datos de métodos de los DataFrames...\n",
      "✅ Cargado y extraído métodos de: CO_42101\n",
      "✅ Cargado y extraído métodos de: NO2_42602\n",
      "✅ Cargado y extraído métodos de: SO2_42401\n",
      "✅ Cargado y extraído métodos de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_unique_methods' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_unique_methods': ['Method Code', 'Method Name', 'Method Type']\n",
      "Primeras 5 filas del DataFrame 'df_unique_methods':\n",
      "   Method Code                                        Method Name Method Type\n",
      "0          554  INSTRUMENTAL - Gas Filter Correlation Thermo E...         FRM\n",
      "1           54              INSTRUMENTAL - NONDISPERSIVE INFRARED         FRM\n",
      "2          593  INSTRUMENTAL - Gas Filter Correlation Teledyne...         FRM\n",
      "3           93  INSTRUMENTAL - GAS FILTER CORRELATION CO ANALYZER         FRM\n",
      "4          588  INSTRUMENTAL - Gas Filter Correlation Ecotech ...         FRM\n",
      "Número total de métodos únicos a cargar: 45\n",
      "Tipos de datos de las columnas en 'df_unique_methods':\n",
      "Method Code     int64\n",
      "Method Name    object\n",
      "Method Type    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_method.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto (donde están tus carpetas de datos) ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path # Guardar el SQL en la misma carpeta base\n",
    "\n",
    "# --- 3. Cargar los DataFrames de contaminantes para extraer los métodos ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_methods = []\n",
    "\n",
    "print(\"Extrayendo datos de métodos de los DataFrames...\")\n",
    "\n",
    "# Columnas esperadas en los CSV para la tabla method\n",
    "expected_method_cols = ['Method Code', 'Method Name', 'Method Type']\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=expected_method_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Añadir a la lista\n",
    "        all_methods.append(df[expected_method_cols])\n",
    "        print(f\"✅ Cargado y extraído métodos de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ADVERTENCIA: Archivo no encontrado para {key} en {path}. Será omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_methods:\n",
    "    print(\"No se pudieron cargar datos de métodos de ningún archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar todos los DataFrames de métodos\n",
    "df_methods = pd.concat(all_methods, ignore_index=True)\n",
    "\n",
    "# Obtener métodos únicos\n",
    "# Usamos todas las columnas del método para definir la unicidad\n",
    "df_unique_methods = df_methods.drop_duplicates(subset=['Method Code', 'Method Name', 'Method Type']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_unique_methods' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_unique_methods': {df_unique_methods.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_unique_methods':\\n{df_unique_methods.head()}\")\n",
    "print(f\"Número total de métodos únicos a cargar: {len(df_unique_methods)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_unique_methods':\\n{df_unique_methods.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_method.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO method (method_code, method_name, method_type) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_unique_methods.iterrows():\n",
    "        # Función para limpiar y manejar NULLs para SQL\n",
    "        def clean_for_sql(value, is_numeric=False):\n",
    "            if pd.isna(value):\n",
    "                return 'NULL'\n",
    "            if is_numeric:\n",
    "                return str(int(value)) if value == value else 'NULL' # Manejar NaN para enteros\n",
    "            return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "\n",
    "        method_code = clean_for_sql(fila['Method Code'], is_numeric=True)\n",
    "        method_name = clean_for_sql(fila['Method Name'])\n",
    "        method_type = clean_for_sql(fila['Method Type'])\n",
    "\n",
    "        linea = f\"({method_code}, {method_name}, {method_type})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_unique_methods) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b34e2d9-3bf9-47b8-a3c2-7466f1e8da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexión a la base de datos...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contraseña de MySQL:  ········\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Motor de conexión a 'calidad_aire' creado exitosamente.\n",
      "Extrayendo datos de estaciones (site_num, state_code, county_code) de los DataFrames horarios...\n",
      "✅ Cargado y extraído estaciones de: CO_42101\n",
      "✅ Cargado y extraído estaciones de: NO2_42602\n",
      "✅ Cargado y extraído estaciones de: SO2_42401\n",
      "✅ Cargado y extraído estaciones de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: Estaciones únicas del CSV antes de unir con location ---\n",
      "Primeras 5 filas:\n",
      "   State Code  County Code  Site Num\n",
      "0           1           73        23\n",
      "1           1           73      1003\n",
      "2           1           73      2059\n",
      "3           1           73      6004\n",
      "4           2           20        18\n",
      "Número total de estaciones únicas en CSV: 1590\n",
      "--------------------------------------------------\n",
      "\n",
      "Obteniendo 'location_id' de la tabla 'location' de MySQL...\n",
      "✅ Cargadas 1590 ubicaciones de la tabla 'location'.\n",
      "Primeras 5 filas de 'df_db_locations':\n",
      "   location_id  state_code  county_code\n",
      "0            1           1           73\n",
      "1            2           1           73\n",
      "2            3           1           73\n",
      "3            4           1           73\n",
      "4            5           2           20\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_final_stations' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_final_stations': ['site_num', 'location_id']\n",
      "Primeras 5 filas del DataFrame 'df_final_stations':\n",
      "   site_num  location_id\n",
      "0        23            1\n",
      "1        23            2\n",
      "2        23            3\n",
      "3        23            4\n",
      "4        23          809\n",
      "Número total de estaciones (site_num con location_id) a cargar: 6752\n",
      "Tipos de datos de las columnas en 'df_final_stations':\n",
      "site_num       int64\n",
      "location_id    int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_station.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass # Importar getpass\n",
    "\n",
    "# --- 1. Configuración de la conexión a la base de datos (USANDO TU MÉTODO) ---\n",
    "print(\"Configurando conexión a la base de datos...\")\n",
    "# Solicitar credenciales de forma segura\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contraseña = getpass(\"Introduce tu contraseña de MySQL: \") # getpass para ocultar la entrada\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\" # Tu base de datos es 'calidad_aire'\n",
    "\n",
    "# Crear motor de conexión con SQLAlchemy\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "    print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 3. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 4. Cargar los DataFrames de contaminantes para extraer Site Num ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_station_data = []\n",
    "\n",
    "print(\"Extrayendo datos de estaciones (site_num, state_code, county_code) de los DataFrames horarios...\")\n",
    "\n",
    "expected_station_cols = ['State Code', 'County Code', 'Site Num'] # Columnas necesarias del CSV\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=expected_station_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        all_station_data.append(df[expected_station_cols])\n",
    "        print(f\"✅ Cargado y extraído estaciones de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ADVERTENCIA: Archivo no encontrado para {key} en {path}. Será omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_station_data:\n",
    "    print(\"No se pudieron cargar datos de estaciones de ningún archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "df_raw_stations = pd.concat(all_station_data, ignore_index=True)\n",
    "\n",
    "# Obtener combinaciones únicas de State Code, County Code, Site Num\n",
    "df_unique_stations_csv = df_raw_stations.drop_duplicates(subset=['State Code', 'County Code', 'Site Num']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: Estaciones únicas del CSV antes de unir con location ---\")\n",
    "print(f\"Primeras 5 filas:\\n{df_unique_stations_csv.head()}\")\n",
    "print(f\"Número total de estaciones únicas en CSV: {len(df_unique_stations_csv)}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 5. Obtener location_id de la base de datos usando el motor SQLAlchemy ---\n",
    "print(\"Obteniendo 'location_id' de la tabla 'location' de MySQL...\")\n",
    "try:\n",
    "    # Leer la tabla 'location' de MySQL\n",
    "    df_db_locations = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", engine)\n",
    "    \n",
    "    # Asegurarse de que los tipos de datos coincidan para el merge\n",
    "    df_db_locations['state_code'] = df_db_locations['state_code'].astype(int)\n",
    "    df_db_locations['county_code'] = df_db_locations['county_code'].astype(int)\n",
    "    \n",
    "    print(f\"✅ Cargadas {len(df_db_locations)} ubicaciones de la tabla 'location'.\")\n",
    "    print(f\"Primeras 5 filas de 'df_db_locations':\\n{df_db_locations.head()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al obtener 'location_id' de la base de datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. Unir los datos de estaciones del CSV con los location_id de la BBDD ---\n",
    "# Cambiar nombres de columnas en df_unique_stations_csv para el merge\n",
    "df_unique_stations_csv = df_unique_stations_csv.rename(columns={\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code'\n",
    "})\n",
    "\n",
    "df_final_stations = pd.merge(\n",
    "    df_unique_stations_csv,\n",
    "    df_db_locations,\n",
    "    on=['state_code', 'county_code'],\n",
    "    how='inner' # Solo queremos estaciones para las que tenemos un location_id válido\n",
    ")\n",
    "\n",
    "# Seleccionar y renombrar columnas para que coincidan con la tabla 'station'\n",
    "df_final_stations = df_final_stations[['Site Num', 'location_id']].rename(columns={'Site Num': 'site_num'})\n",
    "\n",
    "# Eliminar duplicados finales en caso de que múltiples combinaciones state/county/site num apunten a la misma station (no debería pasar con inner join)\n",
    "df_final_stations = df_final_stations.drop_duplicates(subset=['site_num', 'location_id']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_final_stations' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_final_stations': {df_final_stations.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_final_stations':\\n{df_final_stations.head()}\")\n",
    "print(f\"Número total de estaciones (site_num con location_id) a cargar: {len(df_final_stations)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_final_stations':\\n{df_final_stations.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 7. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_station.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO station (site_num, location_id) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_final_stations.iterrows():\n",
    "        site_num_val = int(fila['site_num']) if pd.notna(fila['site_num']) else 'NULL'\n",
    "        location_id_val = int(fila['location_id']) if pd.notna(fila['location_id']) else 'NULL'\n",
    "\n",
    "        linea = f\"({site_num_val}, {location_id_val})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_final_stations) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c17288-d5f8-4750-bfb1-c4f30d547902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexión a la base de datos (solo para referencia, no se usa para leer aquí)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- 1. Configuración de la conexión a la base de datos (USANDO TU MÉTODO) ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Aunque no necesitamos la conexión para GENERAR el SQL, la mantenemos para consistencia\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# y porque es parte de tu flujo de trabajo. No se usará para leer de la BBDD en este script.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigurando conexión a la base de datos (solo para referencia, no se usa para leer aquí)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m usuario \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce tu usuario de MySQL (por defecto \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m contraseña \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce tu contraseña de MySQL: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m host \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce el host (por defecto \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unicornenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unicornenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass\n",
    "\n",
    "# --- 1. Configuración de la conexión a la base de datos (USANDO TU MÉTODO) ---\n",
    "# Aunque no necesitamos la conexión para GENERAR el SQL, la mantenemos para consistencia\n",
    "# y porque es parte de tu flujo de trabajo. No se usará para leer de la BBDD en este script.\n",
    "print(\"Configurando conexión a la base de datos (solo para referencia, no se usa para leer aquí)...\")\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "    print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "    # No salimos aquí porque la conexión no es estrictamente necesaria para generar el SQL\n",
    "    # pero el usuario debe saber si falla.\n",
    "\n",
    "# --- 2. Definir la ruta base de tu proyecto y del archivo CBSA ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Asegúrate que esta ruta sea correcta!\n",
    "cbsa_data_path = os.path.join(base_project_path, 'Daily AQI by CBSA', 'daily_aqi_by_cbsa_2015', 'daily_aqi_by_cbsa_2015.csv')\n",
    "\n",
    "# --- 3. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 4. Cargar el DataFrame de aqi_cbsa ---\n",
    "print(f\"\\nCargando datos para aqi_cbsa desde: {cbsa_data_path}...\")\n",
    "\n",
    "# Columnas esperadas en el CSV para la tabla aqi_cbsa\n",
    "expected_cbsa_cols = [\n",
    "    'CBSA Code', 'CBSA', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "\n",
    "try:\n",
    "    df_aqi_cbsa = pd.read_csv(cbsa_data_path, usecols=expected_cbsa_cols)\n",
    "    df_aqi_cbsa.columns = df_aqi_cbsa.columns.str.strip() # Limpiar nombres de columna\n",
    "    print(f\"✅ Archivo '{os.path.basename(cbsa_data_path)}' cargado exitosamente.\")\n",
    "    print(f\"Total de filas leídas: {len(df_aqi_cbsa)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Archivo no encontrado para aqi_cbsa en {cbsa_data_path}. Verifica la ruta y que el archivo exista.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo aqi_cbsa. Verifica los nombres de las columnas en el CSV.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR inesperado al procesar aqi_cbsa: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Renombrar columnas para que coincidan con la tabla de MySQL\n",
    "df_aqi_cbsa = df_aqi_cbsa.rename(columns={\n",
    "    'CBSA Code': 'cbsa_code',\n",
    "    'CBSA': 'cbsa_name',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "})\n",
    "\n",
    "# Convertir la columna 'date' a formato de fecha\n",
    "df_aqi_cbsa['date'] = pd.to_datetime(df_aqi_cbsa['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Manejar valores NaN/None para inserción SQL (convertir a cadena 'NULL')\n",
    "def handle_null_for_sql(value, is_string=True):\n",
    "    if pd.isna(value):\n",
    "        return 'NULL'\n",
    "    if is_string:\n",
    "        return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "    return str(value) # Para números, no comillas\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_aqi_cbsa' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_aqi_cbsa': {df_aqi_cbsa.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_aqi_cbsa':\\n{df_aqi_cbsa.head()}\")\n",
    "print(f\"Número total de registros a cargar en aqi_cbsa: {len(df_aqi_cbsa)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_aqi_cbsa':\\n{df_aqi_cbsa.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 5. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_aqi_cbsa.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO aqi_cbsa (cbsa_code, cbsa_name, date, aqi, category, defining_parameter, defining_site, sites_reporting) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_aqi_cbsa.iterrows():\n",
    "        cbsa_code_val = handle_null_for_sql(fila['cbsa_code'], is_string=False)\n",
    "        cbsa_name_val = handle_null_for_sql(fila['cbsa_name'])\n",
    "        date_val = handle_null_for_sql(fila['date'])\n",
    "        aqi_val = handle_null_for_sql(fila['aqi'], is_string=False)\n",
    "        category_val = handle_null_for_sql(fila['category'])\n",
    "        defining_parameter_val = handle_null_for_sql(fila['defining_parameter'])\n",
    "        defining_site_val = handle_null_for_sql(fila['defining_site'])\n",
    "        sites_reporting_val = handle_null_for_sql(fila['sites_reporting'], is_string=False)\n",
    "\n",
    "        linea = (\n",
    "            f\"({cbsa_code_val}, {cbsa_name_val}, {date_val}, {aqi_val}, \"\n",
    "            f\"{category_val}, {defining_parameter_val}, {defining_site_val}, {sites_reporting_val})\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_cbsa) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d868c000-21ff-4d99-87b4-480d99548cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando datos para aqi_county desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv...\n",
      "✅ Archivo 'daily_aqi_by_county_2015.csv' cargado exitosamente.\n",
      "Total de filas leídas: 320535\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_aqi_county' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_aqi_county': ['state_code', 'county_code', 'date', 'aqi', 'category', 'defining_parameter', 'defining_site', 'sites_reporting']\n",
      "Primeras 5 filas del DataFrame 'df_aqi_county':\n",
      "   state_code  county_code        date  aqi  category defining_parameter  \\\n",
      "0           1            3  2015-01-03   38      Good              PM2.5   \n",
      "1           1            3  2015-01-06   55  Moderate              PM2.5   \n",
      "2           1            3  2015-01-09   60  Moderate              PM2.5   \n",
      "3           1            3  2015-01-12   52  Moderate              PM2.5   \n",
      "4           1            3  2015-01-15   35      Good              PM2.5   \n",
      "\n",
      "  defining_site  sites_reporting  \n",
      "0   01-003-0010                1  \n",
      "1   01-003-0010                1  \n",
      "2   01-003-0010                1  \n",
      "3   01-003-0010                1  \n",
      "4   01-003-0010                1  \n",
      "Número total de registros a cargar en aqi_county: 320535\n",
      "Tipos de datos de las columnas en 'df_aqi_county':\n",
      "state_code             int64\n",
      "county_code            int64\n",
      "date                  object\n",
      "aqi                    int64\n",
      "category              object\n",
      "defining_parameter    object\n",
      "defining_site         object\n",
      "sites_reporting        int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "✅ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_aqi_county.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# No necesitamos importar create_engine ni getpass de nuevo si el motor ya está definido\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto y del archivo County ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "county_data_path = os.path.join(base_project_path, 'Daily AQI by County', 'daily_aqi_by_county_2015', 'daily_aqi_by_county_2015.csv')\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 3. Cargar el DataFrame de aqi_county ---\n",
    "print(f\"\\nCargando datos para aqi_county desde: {county_data_path}...\")\n",
    "\n",
    "# Columnas esperadas en el CSV para la tabla aqi_county\n",
    "expected_county_cols = [\n",
    "    'State Code', 'County Code', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "\n",
    "try:\n",
    "    df_aqi_county = pd.read_csv(county_data_path, usecols=expected_county_cols)\n",
    "    df_aqi_county.columns = df_aqi_county.columns.str.strip() # Limpiar nombres de columna\n",
    "    print(f\"✅ Archivo '{os.path.basename(county_data_path)}' cargado exitosamente.\")\n",
    "    print(f\"Total de filas leídas: {len(df_aqi_county)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Archivo no encontrado para aqi_county en {county_data_path}. Verifica la ruta y que el archivo exista.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo aqi_county. Verifica los nombres de las columnas en el CSV.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR inesperado al procesar aqi_county: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Renombrar columnas para que coincidan con la tabla de MySQL\n",
    "df_aqi_county = df_aqi_county.rename(columns={\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "}) #\n",
    "\n",
    "# Convertir la columna 'date' a formato de fecha\n",
    "df_aqi_county['date'] = pd.to_datetime(df_aqi_county['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Manejar valores NaN/None para inserción SQL (convertir a cadena 'NULL')\n",
    "def handle_null_for_sql(value, is_string=True):\n",
    "    if pd.isna(value):\n",
    "        return 'NULL'\n",
    "    if is_string:\n",
    "        return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "    return str(value) # Para números, no comillas\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_aqi_county' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_aqi_county': {df_aqi_county.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_aqi_county':\\n{df_aqi_county.head()}\")\n",
    "print(f\"Número total de registros a cargar en aqi_county: {len(df_aqi_county)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_aqi_county':\\n{df_aqi_county.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserción ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_aqi_county.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO aqi_county (state_code, county_code, date, aqi, category, defining_parameter, defining_site, sites_reporting) VALUES\\n\") #\n",
    "    \n",
    "    for i, fila in df_aqi_county.iterrows():\n",
    "        state_code_val = handle_null_for_sql(fila['state_code'], is_string=False)\n",
    "        county_code_val = handle_null_for_sql(fila['county_code'], is_string=False)\n",
    "        date_val = handle_null_for_sql(fila['date'])\n",
    "        aqi_val = handle_null_for_sql(fila['aqi'], is_string=False)\n",
    "        category_val = handle_null_for_sql(fila['category'])\n",
    "        defining_parameter_val = handle_null_for_sql(fila['defining_parameter'])\n",
    "        defining_site_val = handle_null_for_sql(fila['defining_site'])\n",
    "        sites_reporting_val = handle_null_for_sql(fila['sites_reporting'], is_string=False)\n",
    "\n",
    "        linea = (\n",
    "            f\"({state_code_val}, {county_code_val}, {date_val}, {aqi_val}, \"\n",
    "            f\"{category_val}, {defining_parameter_val}, {defining_site_val}, {sites_reporting_val})\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_county) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"✅ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2662cccd-5503-4a5b-ae45-1a1e0d11ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO CARGA MASIVA DE AQI_CBSA ---\n",
      "❌ ERROR al TRUNCATE la tabla 'aqi_cbsa': module 'pandas' has no attribute 'text'\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2015\\daily_aqi_by_cbsa_2015.csv...\n",
      "✅ Año 2015 procesado y cargado. Filas cargadas: 169851. Tiempo: 5.44 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2016\\daily_aqi_by_cbsa_2016.csv...\n",
      "✅ Año 2016 procesado y cargado. Filas cargadas: 170424. Tiempo: 7.36 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2017\\daily_aqi_by_cbsa_2017.csv...\n",
      "✅ Año 2017 procesado y cargado. Filas cargadas: 173601. Tiempo: 9.31 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2018\\daily_aqi_by_cbsa_2018.csv...\n",
      "✅ Año 2018 procesado y cargado. Filas cargadas: 172699. Tiempo: 7.82 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2019\\daily_aqi_by_cbsa_2019.csv...\n",
      "✅ Año 2019 procesado y cargado. Filas cargadas: 170430. Tiempo: 8.20 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2020\\daily_aqi_by_cbsa_2020.csv...\n",
      "✅ Año 2020 procesado y cargado. Filas cargadas: 169779. Tiempo: 8.47 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2021\\daily_aqi_by_cbsa_2021.csv...\n",
      "✅ Año 2021 procesado y cargado. Filas cargadas: 170364. Tiempo: 9.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2022\\daily_aqi_by_cbsa_2022.csv...\n",
      "✅ Año 2022 procesado y cargado. Filas cargadas: 168627. Tiempo: 7.30 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2023\\daily_aqi_by_cbsa_2023.csv...\n",
      "✅ Año 2023 procesado y cargado. Filas cargadas: 169558. Tiempo: 8.94 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2024\\daily_aqi_by_cbsa_2024.csv...\n",
      "✅ Año 2024 procesado y cargado. Filas cargadas: 109683. Tiempo: 5.69 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE AQI_CBSA COMPLETADA ---\n",
      "Total de filas cargadas en 'aqi_cbsa': 1645016\n",
      "Tiempo total de ejecución: 77.59 segundos.\n",
      "❌ ERROR al verificar el conteo de filas en 'aqi_cbsa': module 'pandas' has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass\n",
    "import time # Para medir el tiempo de ejecución\n",
    "\n",
    "# --- 0. Configuración de la conexión a la base de datos (si el motor no está activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o estás ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente sección para crear el 'engine'.\n",
    "# Si el 'engine' de un paso anterior sigue activo, deja esta sección COMENTADA.\n",
    "\n",
    "# print(\"Configurando conexión a la base de datos...\")\n",
    "# usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "# contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "# host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "# bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "# try:\n",
    "#     engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "#     print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "#     print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "#     exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los años a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024 (el rango es exclusivo en el final)\n",
    "\n",
    "# --- 3. Columnas esperadas en los CSV y sus mapeos a la tabla MySQL ---\n",
    "expected_cbsa_cols = [\n",
    "    'CBSA Code', 'CBSA', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "]\n",
    "mysql_col_names = {\n",
    "    'CBSA Code': 'cbsa_code',\n",
    "    'CBSA': 'cbsa_name',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "}\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla antes de la carga masiva (opcional, pero recomendado si es la primera carga completa) ---\n",
    "# Si estás ejecutando esto por primera vez para todos los años, TRUNCATE es necesario.\n",
    "# Si estás añadiendo años nuevos a una tabla ya existente con años anteriores, NO uses TRUNCATE.\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE AQI_CBSA ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(pd.text(\"TRUNCATE TABLE aqi_cbsa;\"))\n",
    "        connection.commit()\n",
    "    print(\"✅ Tabla 'aqi_cbsa' TRUNCATED (vacía) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al TRUNCATE la tabla 'aqi_cbsa': {e}\")\n",
    "    # Considera si quieres salir o continuar si el TRUNCATE falla.\n",
    "    # Por ahora, seguiremos si el error no es crítico.\n",
    "\n",
    "# --- 5. Procesar y cargar datos año por año ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "for year in years:\n",
    "    cbsa_data_dir = os.path.join(base_project_path, 'Daily AQI by CBSA', f'daily_aqi_by_cbsa_{year}')\n",
    "    cbsa_data_file = os.path.join(cbsa_data_dir, f'daily_aqi_by_cbsa_{year}.csv')\n",
    "\n",
    "    print(f\"\\nProcesando y cargando datos para aqi_cbsa del año {year} desde: {cbsa_data_file}...\")\n",
    "    start_time_year = time.time()\n",
    "\n",
    "    try:\n",
    "        # Cargar el CSV\n",
    "        df = pd.read_csv(cbsa_data_file, usecols=expected_cbsa_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Renombrar columnas\n",
    "        df = df.rename(columns=mysql_col_names)\n",
    "        \n",
    "        # Convertir la columna 'date' a formato de fecha\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Opcional: convertir columnas a tipos de datos específicos antes de to_sql\n",
    "        # Esto puede ayudar con la inferencia de tipos de SQLAlchemy\n",
    "        df['cbsa_code'] = df['cbsa_code'].astype('Int64') # Usar Int64 para manejar NaN como NULL\n",
    "        df['aqi'] = df['aqi'].astype('Int64')\n",
    "        df['sites_reporting'] = df['sites_reporting'].astype('Int64')\n",
    "\n",
    "        rows_in_chunk = len(df)\n",
    "        total_rows_loaded += rows_in_chunk\n",
    "\n",
    "        # Usar to_sql para insertar en la base de datos\n",
    "        # if_exists='append' para añadir al final de la tabla\n",
    "        # index=False para no insertar el índice del DataFrame como columna\n",
    "        # chunksize para insertar en lotes (importante para grandes DataFrames)\n",
    "        df.to_sql('aqi_cbsa', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "\n",
    "        end_time_year = time.time()\n",
    "        print(f\"✅ Año {year} procesado y cargado. Filas cargadas: {rows_in_chunk}. Tiempo: {end_time_year - start_time_year:.2f} segundos.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ADVERTENCIA: Archivo no encontrado para {year} en {cbsa_data_file}. Se omitirá este año.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo de {year}. Verifica los nombres de las columnas en el CSV. Se omitirá este año.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar el año {year}: {e}. Se omitirá este año.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE AQI_CBSA COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'aqi_cbsa': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecución: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(pd.text(\"SELECT COUNT(*) FROM aqi_cbsa;\")).scalar()\n",
    "    print(f\"✅ Conteo final de filas en la tabla 'aqi_cbsa' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al verificar el conteo de filas en 'aqi_cbsa': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5828f41c-f899-44cd-8b91-f8f5b51e5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO CARGA MASIVA DE AQI_COUNTY ---\n",
      "✅ Tabla 'aqi_county' TRUNCATED (vacía) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv...\n",
      "✅ Año 2015 procesado y cargado. Filas cargadas: 320535. Tiempo: 14.26 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2016\\daily_aqi_by_county_2016.csv...\n",
      "✅ Año 2016 procesado y cargado. Filas cargadas: 321071. Tiempo: 15.09 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2017\\daily_aqi_by_county_2017.csv...\n",
      "✅ Año 2017 procesado y cargado. Filas cargadas: 326801. Tiempo: 15.53 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2018\\daily_aqi_by_county_2018.csv...\n",
      "✅ Año 2018 procesado y cargado. Filas cargadas: 327543. Tiempo: 15.34 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2019\\daily_aqi_by_county_2019.csv...\n",
      "✅ Año 2019 procesado y cargado. Filas cargadas: 326046. Tiempo: 16.17 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2020\\daily_aqi_by_county_2020.csv...\n",
      "✅ Año 2020 procesado y cargado. Filas cargadas: 325138. Tiempo: 15.10 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2021\\daily_aqi_by_county_2021.csv...\n",
      "✅ Año 2021 procesado y cargado. Filas cargadas: 326540. Tiempo: 15.55 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2022\\daily_aqi_by_county_2022.csv...\n",
      "✅ Año 2022 procesado y cargado. Filas cargadas: 324422. Tiempo: 14.70 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2023\\daily_aqi_by_county_2023.csv...\n",
      "✅ Año 2023 procesado y cargado. Filas cargadas: 325393. Tiempo: 15.61 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2024\\daily_aqi_by_county_2024.csv...\n",
      "✅ Año 2024 procesado y cargado. Filas cargadas: 206919. Tiempo: 9.97 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE AQI_COUNTY COMPLETADA ---\n",
      "Total de filas cargadas en 'aqi_county': 3130408\n",
      "Tiempo total de ejecución: 147.33 segundos.\n",
      "✅ Conteo final de filas en la tabla 'aqi_county' en MySQL: 3130408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text # Importar 'text' de sqlalchemy\n",
    "import time\n",
    "\n",
    "# --- 0. Configuración de la conexión a la base de datos (si el motor no está activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o estás ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente sección para crear el 'engine'.\n",
    "# Si el 'engine' de un paso anterior sigue activo, deja esta sección COMENTADA.\n",
    "\n",
    "# print(\"Configurando conexión a la base de base de datos...\")\n",
    "# usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "# contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "# host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "# bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "# try:\n",
    "#     engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "#     print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "#     print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "#     exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¡Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los años a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "\n",
    "# --- 3. Columnas esperadas en los CSV y sus mapeos a la tabla MySQL ---\n",
    "expected_county_cols = [\n",
    "    'State Code', 'County Code', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "mysql_col_names_county = {\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "} #\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla antes de la carga masiva (opcional, pero recomendado si es la primera carga completa) ---\n",
    "# Si la tabla ya estaba completamente vacía y sabes que esta es la primera carga, puedes ejecutarlo.\n",
    "# Si no, considera omitirlo o usar un DELETE WHERE para un rango de fechas.\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE AQI_COUNTY ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE aqi_county;\")) # Usar text() de sqlalchemy\n",
    "        connection.commit()\n",
    "    print(\"✅ Tabla 'aqi_county' TRUNCATED (vacía) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al TRUNCATE la tabla 'aqi_county': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Asegúrate de que la tabla esté vacía o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos año por año ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "for year in years:\n",
    "    county_data_dir = os.path.join(base_project_path, 'Daily AQI by County', f'daily_aqi_by_county_{year}')\n",
    "    county_data_file = os.path.join(county_data_dir, f'daily_aqi_by_county_{year}.csv')\n",
    "\n",
    "    print(f\"\\nProcesando y cargando datos para aqi_county del año {year} desde: {county_data_file}...\")\n",
    "    start_time_year = time.time()\n",
    "\n",
    "    try:\n",
    "        # Cargar el CSV\n",
    "        df = pd.read_csv(county_data_file, usecols=expected_county_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Renombrar columnas\n",
    "        df = df.rename(columns=mysql_col_names_county)\n",
    "        \n",
    "        # Convertir la columna 'date' a formato de fecha\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Opcional: convertir columnas a tipos de datos específicos antes de to_sql\n",
    "        df['state_code'] = df['state_code'].astype('Int64')\n",
    "        df['county_code'] = df['county_code'].astype('Int64')\n",
    "        df['aqi'] = df['aqi'].astype('Int64')\n",
    "        df['sites_reporting'] = df['sites_reporting'].astype('Int64')\n",
    "\n",
    "        rows_in_chunk = len(df)\n",
    "        total_rows_loaded += rows_in_chunk\n",
    "\n",
    "        # Usar to_sql para insertar en la base de datos\n",
    "        df.to_sql('aqi_county', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "\n",
    "        end_time_year = time.time()\n",
    "        print(f\"✅ Año {year} procesado y cargado. Filas cargadas: {rows_in_chunk}. Tiempo: {end_time_year - start_time_year:.2f} segundos.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ADVERTENCIA: Archivo no encontrado para {year} en {county_data_file}. Se omitirá este año.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo de {year}. Verifica los nombres de las columnas en el CSV. Se omitirá este año.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar el año {year}: {e}. Se omitirá este año.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE AQI_COUNTY COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'aqi_county': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecución: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM aqi_county;\")).scalar() # Usar text() de sqlalchemy\n",
    "    print(f\"✅ Conteo final de filas en la tabla 'aqi_county' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al verificar el conteo de filas en 'aqi_county': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d779f9db-9057-457a-a34e-c1465f6d2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexión a la base de datos...\n",
      "✅ Motor de conexión ya existente y activo.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "✅ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\n",
      "✅ Tabla 'measurement' TRUNCATED (vacía) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "❌ ADVERTENCIA: Archivo no encontrado para CO en 2016 (C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv). Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2016\\hourly_42602_2016.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2016\\hourly_42401_2016.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2016\\hourly_44201_2016.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2017\\hourly_42101_2017.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2017\\hourly_42602_2017.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2017\\hourly_42401_2017.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2017\\hourly_44201_2017.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2018\\hourly_42101_2018.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2018\\hourly_42602_2018.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2018\\hourly_42401_2018.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2018\\hourly_44201_2018.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2019\\hourly_42101_2019.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2019\\hourly_42602_2019.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2019\\hourly_42401_2019.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2019\\hourly_44201_2019.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2020\\hourly_42101_2020.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2020\\hourly_42602_2020.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2020\\hourly_42401_2020.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2020\\hourly_44201_2020.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2021\\hourly_42101_2021.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2021\\hourly_42602_2021.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2021\\hourly_42401_2021.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2021\\hourly_44201_2021.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2022\\hourly_42101_2022.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2022\\hourly_42602_2022.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2022\\hourly_42401_2022.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2022\\hourly_44201_2022.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2023\\hourly_42101_2023.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2023\\hourly_42602_2023.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2023\\hourly_42401_2023.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2023\\hourly_44201_2023.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2024\\hourly_42101_2024.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2024\\hourly_42602_2024.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de NO2 para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2024\\hourly_42401_2024.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de SO2 para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2024\\hourly_44201_2024.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de Ozone para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitirá este archivo.\n",
      "\n",
      "--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\n",
      "Total de filas cargadas en 'measurement': 0\n",
      "Tiempo total de ejecución: 0.21 segundos.\n",
      "✅ Conteo final de filas en la tabla 'measurement' en MySQL: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuración de la conexión a la base de datos (si el motor no está activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o estás ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente sección para crear el 'engine'.\n",
    "\n",
    "print(\"Configurando conexión a la base de datos...\")\n",
    "# Asegurarse de que el motor exista, si no, lo crea.\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None: # Comprueba si 'engine' no está definido o es None\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "        print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"✅ Motor de conexión ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los años y contaminantes a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "pollutants = {\n",
    "    'CO': '42101',\n",
    "    'NO2': '42602',\n",
    "    'SO2': '42401',\n",
    "    'Ozone': '44201'\n",
    "} # Nombres de carpeta y códigos de parámetro\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    # Unir station con location para tener state_code y county_code en df_station\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"✅ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla measurement (opcional, pero recomendado si es la primera carga completa) ---\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE measurement;\"))\n",
    "        connection.commit()\n",
    "    print(\"✅ Tabla 'measurement' TRUNCATED (vacía) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al TRUNCATE la tabla 'measurement': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Asegúrate de que la tabla esté vacía o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos año por año, contaminante por contaminante ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "# Columnas esperadas en los CSV horarios y sus mapeos a la tabla MySQL\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Arithmetic Mean', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Arithmetic Mean': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "} #\n",
    "\n",
    "for year in years:\n",
    "    for pollutant_name, pollutant_code in pollutants.items():\n",
    "        # *** CORRECCIÓN DE LA RUTA DEL ARCHIVO ***\n",
    "        data_file_path = os.path.join(\n",
    "            base_project_path,\n",
    "            f'{pollutant_name}_{pollutant_code}',  # Carpeta principal del contaminante\n",
    "            f'hourly_{pollutant_code}_{year}',     # Subcarpeta del año\n",
    "            f'hourly_{pollutant_code}_{year}.csv'  # Nombre del archivo CSV\n",
    "        ) #\n",
    "\n",
    "        print(f\"\\nProcesando y cargando datos para '{pollutant_name}' del año {year} desde: {data_file_path}...\")\n",
    "        start_time_file = time.time()\n",
    "\n",
    "        if not os.path.exists(data_file_path):\n",
    "            print(f\"❌ ADVERTENCIA: Archivo no encontrado para {pollutant_name} en {year} ({data_file_path}). Se omitirá este archivo.\")\n",
    "            continue\n",
    "\n",
    "        rows_processed_file = 0\n",
    "        \n",
    "        # Usamos chunksize para leer el CSV en partes\n",
    "        try:\n",
    "            for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000): # Leer en chunks de 50,000 filas\n",
    "                chunk.columns = chunk.columns.str.strip() # Limpiar nombres de columna\n",
    "                \n",
    "                # Renombrar columnas para la tabla measurement\n",
    "                chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "                \n",
    "                # Renombrar columnas para los joins\n",
    "                chunk = chunk.rename(columns={\n",
    "                    'State Code': 'state_code',\n",
    "                    'County Code': 'county_code',\n",
    "                    'Site Num': 'site_num',\n",
    "                    'Parameter Code': 'parameter_code',\n",
    "                    'Method Code': 'method_code'\n",
    "                })\n",
    "\n",
    "                # Convertir fechas a formato YYYY-MM-DD\n",
    "                chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                # Realizar los merges para obtener los IDs de las claves foráneas\n",
    "                # Merge con station (que ya tiene location_id, state_code, county_code)\n",
    "                # Usamos how='inner' para solo mantener los registros que tienen una estación válida\n",
    "                chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                        on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "                \n",
    "                # Merge con parameter\n",
    "                chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "                \n",
    "                # Merge con method\n",
    "                chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "                # Seleccionar solo las columnas necesarias para la inserción en 'measurement'\n",
    "                final_chunk = chunk_merged[[\n",
    "                    'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                    'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "                ]]\n",
    "\n",
    "                # Convertir a tipos que acepten NaN (Int64 para enteros, Float64 para floats)\n",
    "                final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "                final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "                final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "                # 'uncertainty' y 'qualifier' son VARCHAR, se manejarán como objetos/strings\n",
    "                \n",
    "                # Insertar el chunk procesado en la base de datos\n",
    "                final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "                \n",
    "                rows_processed_file += len(final_chunk)\n",
    "                total_rows_loaded += len(final_chunk)\n",
    "\n",
    "            end_time_file = time.time()\n",
    "            print(f\"✅ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"❗ ADVERTENCIA: El archivo {data_file_path} está vacío. Se omitirá.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo de {pollutant_name} para {year}. Verifica los nombres de las columnas en el CSV. Se omitirá este archivo.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR inesperado al procesar el archivo de {pollutant_name} para {year}: {e}. Se omitirá este archivo.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecución: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"✅ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13ba5a-052e-476b-815d-39798a06e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexión a la base de datos...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contraseña de MySQL:  ········\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Motor de conexión a 'calidad_aire' creado exitosamente.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "✅ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\n",
      "✅ Tabla 'measurement' TRUNCATED (vacía) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv...\n",
      "✅ Archivo 'hourly_42101_2015.csv' procesado y cargado. Filas cargadas: 15463348. Tiempo: 1149.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv...\n",
      "✅ Archivo 'hourly_42602_2015.csv' procesado y cargado. Filas cargadas: 19565543. Tiempo: 1163.33 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv...\n",
      "✅ Archivo 'hourly_42401_2015.csv' procesado y cargado. Filas cargadas: 15036734. Tiempo: 3927.44 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv...\n",
      "✅ Archivo 'hourly_44201_2015.csv' procesado y cargado. Filas cargadas: 38856199. Tiempo: 2241.09 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "❌ ADVERTENCIA: Archivo no encontrado para CO en 2016 (C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv). Se omitirá este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2016\\hourly_42602_2016.csv...\n",
      "✅ Archivo 'hourly_42602_2016.csv' procesado y cargado. Filas cargadas: 19873458. Tiempo: 1170.05 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2016\\hourly_42401_2016.csv...\n",
      "✅ Archivo 'hourly_42401_2016.csv' procesado y cargado. Filas cargadas: 14893555. Tiempo: 849.91 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2016\\hourly_44201_2016.csv...\n",
      "✅ Archivo 'hourly_44201_2016.csv' procesado y cargado. Filas cargadas: 39634882. Tiempo: 2287.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2017\\hourly_42101_2017.csv...\n",
      "✅ Archivo 'hourly_42101_2017.csv' procesado y cargado. Filas cargadas: 14311759. Tiempo: 838.04 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2017\\hourly_42602_2017.csv...\n",
      "✅ Archivo 'hourly_42602_2017.csv' procesado y cargado. Filas cargadas: 19129388. Tiempo: 1107.67 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2017\\hourly_42401_2017.csv...\n",
      "✅ Archivo 'hourly_42401_2017.csv' procesado y cargado. Filas cargadas: 14110536. Tiempo: 810.59 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2017\\hourly_44201_2017.csv...\n",
      "✅ Archivo 'hourly_44201_2017.csv' procesado y cargado. Filas cargadas: 39079969. Tiempo: 2291.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2018\\hourly_42101_2018.csv...\n",
      "✅ Archivo 'hourly_42101_2018.csv' procesado y cargado. Filas cargadas: 13651003. Tiempo: 802.13 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2018\\hourly_42602_2018.csv...\n",
      "✅ Archivo 'hourly_42602_2018.csv' procesado y cargado. Filas cargadas: 18922705. Tiempo: 1087.13 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2018\\hourly_42401_2018.csv...\n",
      "✅ Archivo 'hourly_42401_2018.csv' procesado y cargado. Filas cargadas: 12917898. Tiempo: 756.15 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2018\\hourly_44201_2018.csv...\n",
      "✅ Archivo 'hourly_44201_2018.csv' procesado y cargado. Filas cargadas: 38571002. Tiempo: 2260.40 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2019\\hourly_42101_2019.csv...\n",
      "✅ Archivo 'hourly_42101_2019.csv' procesado y cargado. Filas cargadas: 12744642. Tiempo: 733.27 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2019\\hourly_42602_2019.csv...\n",
      "✅ Archivo 'hourly_42602_2019.csv' procesado y cargado. Filas cargadas: 18262436. Tiempo: 1063.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2019\\hourly_42401_2019.csv...\n",
      "✅ Archivo 'hourly_42401_2019.csv' procesado y cargado. Filas cargadas: 12620856. Tiempo: 740.31 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2019\\hourly_44201_2019.csv...\n",
      "✅ Archivo 'hourly_44201_2019.csv' procesado y cargado. Filas cargadas: 37042294. Tiempo: 2253.95 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2020\\hourly_42101_2020.csv...\n",
      "✅ Archivo 'hourly_42101_2020.csv' procesado y cargado. Filas cargadas: 12319915. Tiempo: 711.74 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2020\\hourly_42602_2020.csv...\n",
      "✅ Archivo 'hourly_42602_2020.csv' procesado y cargado. Filas cargadas: 18128317. Tiempo: 1055.69 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2020\\hourly_42401_2020.csv...\n",
      "✅ Archivo 'hourly_42401_2020.csv' procesado y cargado. Filas cargadas: 12308536. Tiempo: 740.17 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2020\\hourly_44201_2020.csv...\n",
      "✅ Archivo 'hourly_44201_2020.csv' procesado y cargado. Filas cargadas: 36742548. Tiempo: 2169.41 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2021\\hourly_42101_2021.csv...\n",
      "✅ Archivo 'hourly_42101_2021.csv' procesado y cargado. Filas cargadas: 11770539. Tiempo: 682.92 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2021\\hourly_42602_2021.csv...\n",
      "✅ Archivo 'hourly_42602_2021.csv' procesado y cargado. Filas cargadas: 17584267. Tiempo: 1074.87 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2021\\hourly_42401_2021.csv...\n",
      "✅ Archivo 'hourly_42401_2021.csv' procesado y cargado. Filas cargadas: 11688736. Tiempo: 699.32 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2021\\hourly_44201_2021.csv...\n",
      "✅ Archivo 'hourly_44201_2021.csv' procesado y cargado. Filas cargadas: 35760205. Tiempo: 2176.48 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2022\\hourly_42101_2022.csv...\n",
      "✅ Archivo 'hourly_42101_2022.csv' procesado y cargado. Filas cargadas: 10979660. Tiempo: 643.27 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2022\\hourly_42602_2022.csv...\n",
      "✅ Archivo 'hourly_42602_2022.csv' procesado y cargado. Filas cargadas: 17279558. Tiempo: 1019.80 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2022\\hourly_42401_2022.csv...\n",
      "✅ Archivo 'hourly_42401_2022.csv' procesado y cargado. Filas cargadas: 10892239. Tiempo: 641.74 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2022\\hourly_44201_2022.csv...\n",
      "✅ Archivo 'hourly_44201_2022.csv' procesado y cargado. Filas cargadas: 34722629. Tiempo: 2118.53 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2023\\hourly_42101_2023.csv...\n",
      "✅ Archivo 'hourly_42101_2023.csv' procesado y cargado. Filas cargadas: 10078678. Tiempo: 599.24 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2023\\hourly_42602_2023.csv...\n",
      "✅ Archivo 'hourly_42602_2023.csv' procesado y cargado. Filas cargadas: 16470734. Tiempo: 970.52 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2023\\hourly_42401_2023.csv...\n",
      "✅ Archivo 'hourly_42401_2023.csv' procesado y cargado. Filas cargadas: 10246326. Tiempo: 604.23 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2023\\hourly_44201_2023.csv...\n",
      "✅ Archivo 'hourly_44201_2023.csv' procesado y cargado. Filas cargadas: 34035669. Tiempo: 2045.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2024\\hourly_42101_2024.csv...\n",
      "✅ Archivo 'hourly_42101_2024.csv' procesado y cargado. Filas cargadas: 5445891. Tiempo: 322.46 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2024\\hourly_42602_2024.csv...\n",
      "✅ Archivo 'hourly_42602_2024.csv' procesado y cargado. Filas cargadas: 8831965. Tiempo: 522.05 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2024\\hourly_42401_2024.csv...\n",
      "✅ Archivo 'hourly_42401_2024.csv' procesado y cargado. Filas cargadas: 5758121. Tiempo: 341.23 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del año 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2024\\hourly_44201_2024.csv...\n",
      "✅ Archivo 'hourly_44201_2024.csv' procesado y cargado. Filas cargadas: 19376059. Tiempo: 1182.93 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\n",
      "Total de filas cargadas en 'measurement': 755108799\n",
      "Tiempo total de ejecución: 47853.20 segundos.\n",
      "✅ Conteo final de filas en la tabla 'measurement' en MySQL: 755108799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuración de la conexión a la base de datos (si el motor no está activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o estás ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente sección para crear el 'engine'.\n",
    "\n",
    "print(\"Configurando conexión a la base de datos...\")\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None: # Comprueba si 'engine' no está definido o es None\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "        print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"✅ Motor de conexión ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los años y contaminantes a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "pollutants = {\n",
    "    'CO': '42101',\n",
    "    'NO2': '42602',\n",
    "    'SO2': '42401',\n",
    "    'Ozone': '44201'\n",
    "} # Nombres de carpeta y códigos de parámetro\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    # Unir station con location para tener state_code y county_code en df_station\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"✅ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla measurement (opcional, pero recomendado si es la primera carga completa) ---\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE measurement;\"))\n",
    "        connection.commit()\n",
    "    print(\"✅ Tabla 'measurement' TRUNCATED (vacía) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al TRUNCATE la tabla 'measurement': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Asegúrate de que la tabla esté vacía o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos año por año, contaminante por contaminante ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Sample Measurement', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Sample Measurement': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "}\n",
    "\n",
    "for year in years:\n",
    "    for pollutant_name, pollutant_code in pollutants.items():\n",
    "        data_file_path = os.path.join(\n",
    "            base_project_path,\n",
    "            f'{pollutant_name}_{pollutant_code}',\n",
    "            f'hourly_{pollutant_code}_{year}',\n",
    "            f'hourly_{pollutant_code}_{year}.csv'\n",
    "        )\n",
    "\n",
    "        print(f\"\\nProcesando y cargando datos para '{pollutant_name}' del año {year} desde: {data_file_path}...\")\n",
    "        start_time_file = time.time()\n",
    "\n",
    "        if not os.path.exists(data_file_path):\n",
    "            print(f\"❌ ADVERTENCIA: Archivo no encontrado para {pollutant_name} en {year} ({data_file_path}). Se omitirá este archivo.\")\n",
    "            continue\n",
    "\n",
    "        rows_processed_file = 0\n",
    "        \n",
    "        try:\n",
    "            for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000,\n",
    "                                     on_bad_lines='skip', low_memory=False):\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "\n",
    "                chunk.columns = chunk.columns.str.strip()\n",
    "                \n",
    "                chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "                \n",
    "                chunk = chunk.rename(columns={\n",
    "                    'State Code': 'state_code',\n",
    "                    'County Code': 'county_code',\n",
    "                    'Site Num': 'site_num',\n",
    "                    'Parameter Code': 'parameter_code',\n",
    "                    'Method Code': 'method_code'\n",
    "                })\n",
    "\n",
    "                chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                        on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "                \n",
    "                chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "                \n",
    "                chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "                # *** MODIFICACIÓN AQUÍ: Añadir .copy() para evitar SettingWithCopyWarning ***\n",
    "                final_chunk = chunk_merged[[\n",
    "                    'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                    'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "                ]].copy()\n",
    "\n",
    "                final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "                final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "                final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "                \n",
    "                final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "                \n",
    "                rows_processed_file += len(final_chunk)\n",
    "                total_rows_loaded += len(final_chunk)\n",
    "\n",
    "            end_time_file = time.time()\n",
    "            print(f\"✅ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"❗ ADVERTENCIA: El archivo {data_file_path} está vacío o no contiene datos válidos. Se omitirá.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo de {pollutant_name} para {year}. Esto no debería pasar con 'Sample Measurement'. Por favor, verifica el CSV. Se omitirá este archivo.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR inesperado al procesar el archivo de {pollutant_name} para {year}: {e}. Se omitirá este archivo.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecución: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"✅ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629aaf30-604b-4742-9364-561998f6081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contraseña de MySQL:  ········\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexión a la base de datos...\n",
      "✅ Motor de conexión ya existente y activo.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "✅ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA DEL ARCHIVO FALTANTE ---\n",
      "⚠️ ADVERTENCIA: La tabla 'measurement' NO será truncada. Los datos se añadirán al final.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del año 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "❌ ERROR inesperado al procesar el archivo de CO para 2016: Usecols do not match columns, columns expected but not found: ['Units of Measure', 'State Code', 'County Code', 'Qualifier', 'Method Code', 'Date Local', 'Date of Last Change', 'MDL', 'POC', 'Sample Measurement', 'Parameter Code', 'Uncertainty', 'Site Num'].\n",
      "\n",
      "--- CARGA DEL ARCHIVO FALTANTE COMPLETADA ---\n",
      "Total de filas añadidas en 'measurement': 0\n",
      "Tiempo total de ejecución para este archivo: 0.00 segundos.\n",
      "✅ Conteo final de filas en la tabla 'measurement' en MySQL: 755108799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuración de la conexión a la base de datos (si el motor no está activo) ---\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \")  or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "\n",
    "# Crea el motor de conexión\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Configurando conexión a la base de datos...\")\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None:\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contraseña = getpass(\"Introduce tu contraseña de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contraseña}@{host}/{bd}\")\n",
    "        print(f\"✅ Motor de conexión a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"✅ Motor de conexión ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al crear el motor de conexión: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los parámetros de conexión.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Asegúrate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir el contaminante y año ESPECÍFICO a procesar ---\n",
    "# *** SOLO PARA CARGAR EL ARCHIVO FALTANTE DE CO 2016 ***\n",
    "specific_year = 2016\n",
    "specific_pollutant_name = 'CO'\n",
    "specific_pollutant_code = '42101' #\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"✅ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. NO TRUNCAR la tabla measurement (ya está cargada casi por completo) ---\n",
    "print(\"\\n--- INICIANDO CARGA DEL ARCHIVO FALTANTE ---\")\n",
    "print(\"⚠️ ADVERTENCIA: La tabla 'measurement' NO será truncada. Los datos se añadirán al final.\")\n",
    "\n",
    "\n",
    "# --- 5. Procesar y cargar el archivo específico ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Sample Measurement', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Sample Measurement': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "}\n",
    "\n",
    "# Construye la ruta para el archivo de CO 2016\n",
    "data_file_path = os.path.join(\n",
    "    base_project_path,\n",
    "    f'{specific_pollutant_name}_{specific_pollutant_code}',\n",
    "    f'hourly_{specific_pollutant_code}_{specific_year}',\n",
    "    f'hourly_{specific_pollutant_code}_{specific_year}.csv' # Esperamos el nombre corregido\n",
    ")\n",
    "\n",
    "print(f\"\\nProcesando y cargando datos para '{specific_pollutant_name}' del año {specific_year} desde: {data_file_path}...\")\n",
    "start_time_file = time.time()\n",
    "\n",
    "if not os.path.exists(data_file_path):\n",
    "    print(f\"❌ ERROR: El archivo '{os.path.basename(data_file_path)}' aún NO se encuentra en la ruta esperada. Por favor, asegúrate de haberlo RENOMBRADO a '{os.path.basename(data_file_path)}' y colocado en: '{os.path.dirname(data_file_path)}'.\")\n",
    "else:\n",
    "    rows_processed_file = 0\n",
    "    try:\n",
    "        for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000,\n",
    "                                 on_bad_lines='skip', low_memory=False):\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            \n",
    "            chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "            \n",
    "            chunk = chunk.rename(columns={\n",
    "                'State Code': 'state_code',\n",
    "                'County Code': 'county_code',\n",
    "                'Site Num': 'site_num',\n",
    "                'Parameter Code': 'parameter_code',\n",
    "                'Method Code': 'method_code'\n",
    "            })\n",
    "\n",
    "            chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                    on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "            \n",
    "            chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "            \n",
    "            chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "            final_chunk = chunk_merged[[\n",
    "                'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "            ]].copy()\n",
    "\n",
    "            final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "            final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "            final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "            \n",
    "            final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "            \n",
    "            rows_processed_file += len(final_chunk)\n",
    "            total_rows_loaded += len(final_chunk)\n",
    "\n",
    "        end_time_file = time.time()\n",
    "        print(f\"✅ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"❗ ADVERTENCIA: El archivo {data_file_path} está vacío o no contiene datos válidos. Se omitirá.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"❌ ERROR: Columna '{e}' no encontrada en el archivo de {specific_pollutant_name} para {specific_year}. Por favor, verifica el CSV. Se omitirá este archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR inesperado al procesar el archivo de {specific_pollutant_name} para {specific_year}: {e}.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA DEL ARCHIVO FALTANTE COMPLETADA ---\")\n",
    "print(f\"Total de filas añadidas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecución para este archivo: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"✅ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da3afb-39c7-4b56-b342-e4b0270db382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unicornenv)",
   "language": "python",
   "name": "unicornenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
