{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83be5f93-a06f-44d2-91a5-373ec21531d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\anaconda3\\envs\\unicornenv\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3d00c6-141f-4fe9-8406-a90cdc0e0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a63e85-cf80-40ff-8b3c-c6bdd33d3cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verificando ruta...\n",
      "Ruta completa: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv\n",
      "Existe la carpeta? True\n",
      "Existe el archivo? True\n",
      "Archivo le√≠do exitosamente.\n",
      "Primeras 5 filas:\n",
      "  State Name county Name  State Code  County Code        Date  AQI  Category  \\\n",
      "0    Alabama     Baldwin           1            3  2015-01-03   38      Good   \n",
      "1    Alabama     Baldwin           1            3  2015-01-06   55  Moderate   \n",
      "2    Alabama     Baldwin           1            3  2015-01-09   60  Moderate   \n",
      "3    Alabama     Baldwin           1            3  2015-01-12   52  Moderate   \n",
      "4    Alabama     Baldwin           1            3  2015-01-15   35      Good   \n",
      "\n",
      "  Defining Parameter Defining Site  Number of Sites Reporting  \n",
      "0              PM2.5   01-003-0010                          1  \n",
      "1              PM2.5   01-003-0010                          1  \n",
      "2              PM2.5   01-003-0010                          1  \n",
      "3              PM2.5   01-003-0010                          1  \n",
      "4              PM2.5   01-003-0010                          1  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ruta que estamos intentando usar\n",
    "file_path = \"C:\\\\Users\\\\mjcd1\\\\Desktop\\\\CURSOS\\\\Python Unicorn\\\\Proyecto Integrador Final Calidad del Aire USA\\\\Air Quality Data Project\\\\Daily AQI by County\\\\daily_aqi_by_county_2015\\\\daily_aqi_by_county_2015.csv\"\n",
    "\n",
    "\n",
    "# Mostrar detalles de la ruta\n",
    "print(\"Verificando ruta...\")\n",
    "print(f\"Ruta completa: {file_path}\")\n",
    "print(f\"Existe la carpeta? {os.path.exists(os.path.dirname(file_path))}\")\n",
    "print(f\"Existe el archivo? {os.path.isfile(file_path)}\")\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    import pandas as pd\n",
    "    df_aqi_county_2015 = pd.read_csv(file_path, encoding='utf-8')\n",
    "    print(\"Archivo le√≠do exitosamente.\")\n",
    "    print(\"Primeras 5 filas:\")\n",
    "    print(df_aqi_county_2015.head())\n",
    "else:\n",
    "    print(\"El archivo no se encuentra. Por favor, verifica la ruta o el nombre del archivo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d604b5c-14f1-4e4c-afbf-6ba38b2a0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv ‚Üí CO_42101\n",
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2015\\daily_aqi_by_cbsa_2015.csv ‚Üí Daily AQI by CBSA\n",
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv ‚Üí Daily AQI by County\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv ‚Üí NO2_42602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv ‚Üí Ozone_44201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\1943880471.py:23: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(ruta_csv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Cargado: C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv ‚Üí SO2_42401\n",
      "\n",
      "‚úÖ 6 DataFrames cargados (uno por c√≥digo)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "ruta_base = r\"C:\\Users\\mjcd1\\Desktop\\CURSOS\\Python Unicorn\\Proyecto Integrador Final Calidad del Aire USA\\Air Quality Data Project\"\n",
    "dataframes_por_codigo = {}\n",
    "\n",
    "for carpeta_codigo in os.listdir(ruta_base):\n",
    "    ruta_codigo = os.path.join(ruta_base, carpeta_codigo)\n",
    "    \n",
    "    if os.path.isdir(ruta_codigo):\n",
    "        # Buscamos la subcarpeta del a√±o 2015\n",
    "        subcarpeta_2015 = [d for d in os.listdir(ruta_codigo) if os.path.isdir(os.path.join(ruta_codigo, d)) and '2015' in d]\n",
    "        \n",
    "        if subcarpeta_2015:\n",
    "            ruta_2015 = os.path.join(ruta_codigo, subcarpeta_2015[0])\n",
    "            \n",
    "            # Buscamos el CSV dentro de esa subcarpeta\n",
    "            archivos_csv = [f for f in os.listdir(ruta_2015) if f.endswith('.csv')]\n",
    "            \n",
    "            if archivos_csv:\n",
    "                ruta_csv = os.path.join(ruta_2015, archivos_csv[0])\n",
    "                \n",
    "                df = pd.read_csv(ruta_csv)\n",
    "                dataframes_por_codigo[carpeta_codigo] = df\n",
    "                \n",
    "                print(f\"üì• Cargado: {ruta_csv} ‚Üí {carpeta_codigo}\")\n",
    "\n",
    "print(f\"\\n‚úÖ {len(dataframes_por_codigo)} DataFrames cargados (uno por c√≥digo)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f11da9-e1ca-44cf-b72b-4bd05fbf0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CO_42101', 'Daily AQI by CBSA', 'Daily AQI by County', 'NO2_42602', 'Ozone_44201', 'SO2_42401'])\n"
     ]
    }
   ],
   "source": [
    "print(dataframes_por_codigo.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7b4d52-6f16-474a-95a7-5405f1165081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42101    2  33.553056   \n",
      "1           1           73        23           42101    2  33.553056   \n",
      "2           1           73        23           42101    2  33.553056   \n",
      "3           1           73        23           42101    2  33.553056   \n",
      "4           1           73        23           42101    2  33.553056   \n",
      "\n",
      "   Longitude  Datum   Parameter Name  Date Local  ...   Units of Measure  \\\n",
      "0    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "1    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "2    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "3    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "4    -86.815  WGS84  Carbon monoxide  2015-01-01  ...  Parts per million   \n",
      "\n",
      "    MDL Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0  0.04         NaN        NaN         FRM          554   \n",
      "1  0.04         NaN        NaN         FRM          554   \n",
      "2  0.04         NaN        NaN         FRM          554   \n",
      "3  0.04         NaN        NaN         FRM          554   \n",
      "4  0.04         NaN        NaN         FRM          554   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "1  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "2  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "3  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "4  INSTRUMENTAL - Gas Filter Correlation Thermo E...    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['CO_42101']\n",
    "print(df.head())  # Muestra las primeras 5 filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92d13c2c-8bc4-47d4-9599-cfdfe0f7fc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           CBSA  CBSA Code        Date  AQI Category Defining Parameter  \\\n",
      "0  Aberdeen, SD      10100  2015-01-03   35     Good              PM2.5   \n",
      "1  Aberdeen, SD      10100  2015-01-06   38     Good              PM2.5   \n",
      "2  Aberdeen, SD      10100  2015-01-09   33     Good              PM2.5   \n",
      "3  Aberdeen, SD      10100  2015-01-12   35     Good              PM2.5   \n",
      "4  Aberdeen, SD      10100  2015-01-15   28     Good              PM2.5   \n",
      "\n",
      "  Defining Site  Number of Sites Reporting  \n",
      "0   46-013-0003                          1  \n",
      "1   46-013-0003                          1  \n",
      "2   46-013-0003                          1  \n",
      "3   46-013-0003                          1  \n",
      "4   46-013-0003                          1  \n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Daily AQI by CBSA']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476897d8-39a9-4a02-ac3a-a9b3728e26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State Name county Name  State Code  County Code        Date  AQI  Category  \\\n",
      "0    Alabama     Baldwin           1            3  2015-01-03   38      Good   \n",
      "1    Alabama     Baldwin           1            3  2015-01-06   55  Moderate   \n",
      "2    Alabama     Baldwin           1            3  2015-01-09   60  Moderate   \n",
      "3    Alabama     Baldwin           1            3  2015-01-12   52  Moderate   \n",
      "4    Alabama     Baldwin           1            3  2015-01-15   35      Good   \n",
      "\n",
      "  Defining Parameter Defining Site  Number of Sites Reporting  \n",
      "0              PM2.5   01-003-0010                          1  \n",
      "1              PM2.5   01-003-0010                          1  \n",
      "2              PM2.5   01-003-0010                          1  \n",
      "3              PM2.5   01-003-0010                          1  \n",
      "4              PM2.5   01-003-0010                          1  \n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Daily AQI by County']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "249e50c7-9337-4cc7-9989-a0d9e7e90f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42602    1  33.553056   \n",
      "1           1           73        23           42602    1  33.553056   \n",
      "2           1           73        23           42602    1  33.553056   \n",
      "3           1           73        23           42602    1  33.553056   \n",
      "4           1           73        23           42602    1  33.553056   \n",
      "\n",
      "   Longitude  Datum          Parameter Name  Date Local  ...  \\\n",
      "0    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "1    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "2    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "3    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "4    -86.815  WGS84  Nitrogen dioxide (NO2)  2015-01-01  ...   \n",
      "\n",
      "    Units of Measure  MDL Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "1  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "2  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "3  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "4  Parts per billion  0.1         NaN        NaN         FEM          200   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "1  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "2  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "3  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "4  Teledyne-API Model 200EUP or T200UP - Photolyt...    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['NO2_42602']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ffc88e0-1449-4909-9de5-a0a6d1bcc8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1            3        10           44201    1  30.497478   \n",
      "1           1            3        10           44201    1  30.497478   \n",
      "2           1            3        10           44201    1  30.497478   \n",
      "3           1            3        10           44201    1  30.497478   \n",
      "4           1            3        10           44201    1  30.497478   \n",
      "\n",
      "   Longitude  Datum Parameter Name  Date Local  ...   Units of Measure    MDL  \\\n",
      "0 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "1 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "2 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "3 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "4 -87.880258  NAD83          Ozone  2015-03-01  ...  Parts per million  0.005   \n",
      "\n",
      "  Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0         NaN        NaN         FEM           47   \n",
      "1         NaN        NaN         FEM           47   \n",
      "2         NaN        NaN         FEM           47   \n",
      "3         NaN        NaN         FEM           47   \n",
      "4         NaN        NaN         FEM           47   \n",
      "\n",
      "                   Method Name State Name County Name  Date of Last Change  \n",
      "0  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "1  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "2  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "3  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "4  INSTRUMENTAL - ULTRA VIOLET    Alabama     Baldwin           2015-05-19  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['Ozone_44201']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366d3a48-fe08-4a68-afaf-961f5b11ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   State Code  County Code  Site Num  Parameter Code  POC   Latitude  \\\n",
      "0           1           73        23           42401    2  33.553056   \n",
      "1           1           73        23           42401    2  33.553056   \n",
      "2           1           73        23           42401    2  33.553056   \n",
      "3           1           73        23           42401    2  33.553056   \n",
      "4           1           73        23           42401    2  33.553056   \n",
      "\n",
      "   Longitude  Datum  Parameter Name  Date Local  ...   Units of Measure  MDL  \\\n",
      "0    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "1    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "2    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "3    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "4    -86.815  WGS84  Sulfur dioxide  2015-01-01  ...  Parts per billion  0.2   \n",
      "\n",
      "  Uncertainty  Qualifier Method Type  Method Code  \\\n",
      "0         NaN        NaN         FEM          560   \n",
      "1         NaN        NaN         FEM          560   \n",
      "2         NaN        NaN         FEM          560   \n",
      "3         NaN        NaN         FEM          560   \n",
      "4         NaN        NaN         FEM          560   \n",
      "\n",
      "                                         Method Name State Name County Name  \\\n",
      "0  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "1  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "2  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "3  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "4  INSTRUMENTAL - Pulsed Fluorescent 43C-TLE/43i-TLE    Alabama   Jefferson   \n",
      "\n",
      "   Date of Last Change  \n",
      "0           2015-05-26  \n",
      "1           2015-05-26  \n",
      "2           2015-05-26  \n",
      "3           2015-05-26  \n",
      "4           2015-05-26  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dataframes_por_codigo['SO2_42401']\n",
    "print(df.head())  # Muestra las primeras 5 filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b389620b-5615-4fb5-a85b-f8d9d0abfdaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            9002938\n",
       "Qualifier              8775861\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Ozone_44201'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9878a2e2-f39e-40cb-a926-9d81c410f26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            3722618\n",
       "Qualifier              3580007\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['SO2_42401'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f62a737-d351-4d2c-a932-c1febdb5b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            3549185\n",
       "Qualifier              3436967\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['NO2_42602'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "075efd66-9a66-4142-ae42-d7a724b2c9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Name                   0\n",
       "county Name                  0\n",
       "State Code                   0\n",
       "County Code                  0\n",
       "Date                         0\n",
       "AQI                          0\n",
       "Category                     0\n",
       "Defining Parameter           0\n",
       "Defining Site                0\n",
       "Number of Sites Reporting    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Daily AQI by County'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681f497c-7c04-45c9-bdcc-309e833908f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBSA                         0\n",
       "CBSA Code                    0\n",
       "Date                         0\n",
       "AQI                          0\n",
       "Category                     0\n",
       "Defining Parameter           0\n",
       "Defining Site                0\n",
       "Number of Sites Reporting    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['Daily AQI by CBSA'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c114d26-a9fa-427e-b623-e40b66df2dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State Code                   0\n",
       "County Code                  0\n",
       "Site Num                     0\n",
       "Parameter Code               0\n",
       "POC                          0\n",
       "Latitude                     0\n",
       "Longitude                    0\n",
       "Datum                        0\n",
       "Parameter Name               0\n",
       "Date Local                   0\n",
       "Time Local                   0\n",
       "Date GMT                     0\n",
       "Time GMT                     0\n",
       "Sample Measurement           0\n",
       "Units of Measure             0\n",
       "MDL                          0\n",
       "Uncertainty            2450576\n",
       "Qualifier              2382483\n",
       "Method Type                  0\n",
       "Method Code                  0\n",
       "Method Name                  0\n",
       "State Name                   0\n",
       "County Name                  0\n",
       "Date of Last Change          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_por_codigo['CO_42101'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bd80f40-7bf3-494c-9c38-47b0fef76144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        State Name            County Name  location_id\n",
      "0          Alabama              Jefferson            1\n",
      "1           Alaska             Anchorage             2\n",
      "2           Alaska  Fairbanks North Star             3\n",
      "3          Arizona               Maricopa            4\n",
      "4          Arizona                   Pima            5\n",
      "..             ...                    ...          ...\n",
      "900       Virginia                    NaN          901\n",
      "901     Washington                    NaN          902\n",
      "902  West Virginia                    NaN          903\n",
      "903      Wisconsin                    NaN          904\n",
      "904        Wyoming                    NaN          905\n",
      "\n",
      "[905 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Lista de DataFrames que contienen columnas de ubicaci√≥n\n",
    "dfs = [\n",
    "    dataframes_por_codigo['CO_42101'],\n",
    "    dataframes_por_codigo['NO2_42602'],\n",
    "    dataframes_por_codigo['SO2_42401'],\n",
    "    dataframes_por_codigo['Ozone_44201'],\n",
    "    dataframes_por_codigo['Daily AQI by County']\n",
    "]\n",
    "\n",
    "# Unir todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extraer combinaciones √∫nicas de estado y condado\n",
    "ubicaciones_unicas = df_combinado[['State Name', 'County Name']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Asignar un ID artificial\n",
    "ubicaciones_unicas['location_id'] = ubicaciones_unicas.index + 1\n",
    "\n",
    "# Mostrar resultado\n",
    "print(ubicaciones_unicas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac1cdd-9d89-4150-a2ee-7aa8d3ef7460",
   "metadata": {},
   "source": [
    "## **üêç Funci√≥n en Python para exportar sentencias SQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8641079-4999-4e4c-9256-eb32df2017ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando DataFrames de CO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:13: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_co = pd.read_csv(os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CO cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de NO2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:20: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_no2 = pd.read_csv(os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO2 cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de SO2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:27: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_so2 = pd.read_csv(os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SO2 cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de Ozono...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjcd1\\AppData\\Local\\Temp\\ipykernel_18184\\2288507089.py:34: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_ozone = pd.read_csv(os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv')) #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ozono cargado. Columnas: ['State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC', 'Latitude', 'Longitude', 'Datum', 'Parameter Name', 'Date Local', 'Time Local', 'Date GMT', 'Time GMT', 'Sample Measurement', 'Units of Measure', 'MDL', 'Uncertainty', 'Qualifier', 'Method Type', 'Method Code', 'Method Name', 'State Name', 'County Name', 'Date of Last Change']\n",
      "Cargando DataFrames de Daily AQI by County (para informaci√≥n de condados sin Lat/Lon)...\n",
      "Daily AQI by County cargado. Columnas: ['State Name', 'county Name', 'State Code', 'County Code', 'Date', 'AQI', 'Category', 'Defining Parameter', 'Defining Site', 'Number of Sites Reporting']\n",
      "\n",
      "Iniciando la exportaci√≥n SQL para la tabla 'location'...\n",
      "INFO: Excluyendo 'Daily AQI by County' de la concatenaci√≥n para 'location' ya que no contiene todas las columnas de ubicaci√≥n (Latitude, Longitude, Datum).\n",
      "\n",
      "--- DEPURATION: DataFrame 'ubicaciones' antes de exportar ---\n",
      "Columnas finales del DataFrame 'ubicaciones': ['location_id', 'State Name', 'State Code', 'County Name', 'County Code', 'Latitude', 'Longitude', 'Datum']\n",
      "Primeras 10 filas del DataFrame 'ubicaciones':\n",
      "   location_id State Name  State Code            County Name  County Code  \\\n",
      "0            1    Alabama           1              Jefferson           73   \n",
      "1            2    Alabama           1              Jefferson           73   \n",
      "2            3    Alabama           1              Jefferson           73   \n",
      "3            4    Alabama           1              Jefferson           73   \n",
      "4            5     Alaska           2             Anchorage            20   \n",
      "5            6     Alaska           2  Fairbanks North Star            90   \n",
      "6            7    Arizona           4               Maricopa           13   \n",
      "7            8    Arizona           4               Maricopa           13   \n",
      "8            9    Arizona           4               Maricopa           13   \n",
      "9           10    Arizona           4               Maricopa           13   \n",
      "\n",
      "    Latitude   Longitude  Datum  \n",
      "0  33.553056  -86.815000  WGS84  \n",
      "1  33.485556  -86.915000  WGS84  \n",
      "2  33.521427  -86.844112  WGS84  \n",
      "3  33.565278  -86.796389  WGS84  \n",
      "4  61.205861 -149.824602  WGS84  \n",
      "5  64.845690 -147.727413  WGS84  \n",
      "6  33.483780 -112.142560  NAD83  \n",
      "7  33.410180 -111.865360  NAD83  \n",
      "8  33.560310 -112.066190  NAD83  \n",
      "9  33.574530 -112.191930  NAD83  \n",
      "N√∫mero de filas en 'ubicaciones': 1590\n",
      "Tipos de datos de las columnas en 'ubicaciones':\n",
      "location_id      int64\n",
      "State Name      object\n",
      "State Code       int64\n",
      "County Name     object\n",
      "County Code      int64\n",
      "Latitude       float64\n",
      "Longitude      float64\n",
      "Datum           object\n",
      "dtype: object\n",
      "Conteo de nulos en columnas clave de 'ubicaciones':\n",
      "State Code     0\n",
      "County Code    0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "Datum          0\n",
      "dtype: int64\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Archivo 'insert_locations.sql' generado con 1590 ubicaciones completas.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' #\n",
    "\n",
    "# --- 2. Cargar los DataFrames necesarios para la funci√≥n ---\n",
    "dataframes_por_codigo = {}\n",
    "\n",
    "try:\n",
    "    print(\"Cargando DataFrames de CO...\")\n",
    "    # Eliminado nrows=100 para cargar todos los datos\n",
    "    df_co = pd.read_csv(os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv')) #\n",
    "    df_co.columns = df_co.columns.str.strip() # Limpiar nombres de columna\n",
    "    dataframes_por_codigo['CO_42101'] = df_co\n",
    "    print(f\"CO cargado. Columnas: {df_co.columns.tolist()}\") #\n",
    "\n",
    "    print(\"Cargando DataFrames de NO2...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_no2 = pd.read_csv(os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv')) #\n",
    "    df_no2.columns = df_no2.columns.str.strip()\n",
    "    dataframes_por_codigo['NO2_42602'] = df_no2\n",
    "    print(f\"NO2 cargado. Columnas: {df_no2.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de SO2...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_so2 = pd.read_csv(os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv')) #\n",
    "    df_so2.columns = df_so2.columns.str.strip()\n",
    "    dataframes_por_codigo['SO2_42401'] = df_so2\n",
    "    print(f\"SO2 cargado. Columnas: {df_so2.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de Ozono...\")\n",
    "    # Eliminado nrows=100\n",
    "    df_ozone = pd.read_csv(os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv')) #\n",
    "    df_ozone.columns = df_ozone.columns.str.strip()\n",
    "    dataframes_por_codigo['Ozone_44201'] = df_ozone\n",
    "    print(f\"Ozono cargado. Columnas: {df_ozone.columns.tolist()}\")\n",
    "\n",
    "    print(\"Cargando DataFrames de Daily AQI by County (para informaci√≥n de condados sin Lat/Lon)...\")\n",
    "    # Este DF se carga pero NO se usar√° para la tabla 'location' si 'location' requiere Lat/Lon/Datum.\n",
    "    # Si lo necesitas para otra dimensi√≥n (ej. dim_geografia), aseg√∫rate de usarlo all√≠.\n",
    "    # Eliminado nrows=100\n",
    "    df_daily_county = pd.read_csv(os.path.join(base_project_path, 'Daily AQI by County', 'daily_aqi_by_county_2015', 'daily_aqi_by_county_2015.csv')) #\n",
    "    df_daily_county.columns = df_daily_county.columns.str.strip()\n",
    "    dataframes_por_codigo['Daily AQI by County'] = df_daily_county\n",
    "    print(f\"Daily AQI by County cargado. Columnas: {df_daily_county.columns.tolist()}\") #\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: No se encontr√≥ el archivo: {e}\")\n",
    "    print(\"Por favor, verifica que la ruta base y las rutas de los archivos CSV sean correctas.\")\n",
    "    raise SystemExit # Detiene la ejecuci√≥n si no se encuentran los archivos\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al cargar los Dataframes: {e}\")\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# --- 3. Definici√≥n de la funci√≥n exportar_sql_completo ---\n",
    "def exportar_sql_completo(dataframes_por_codigo, nombre_archivo=\"insert_locations.sql\"):\n",
    "    print(\"\\nIniciando la exportaci√≥n SQL para la tabla 'location'...\")\n",
    "    \n",
    "    # Definimos todas las columnas que esperamos tener en el DataFrame final para la tabla `location`\n",
    "    expected_location_cols = [\n",
    "        'State Name', 'State Code', 'County Name', 'County Code',\n",
    "        'Latitude', 'Longitude', 'Datum'\n",
    "    ]\n",
    "\n",
    "    dfs_to_concat = []\n",
    "    # ITERACI√ìN CLAVE: EXCLUIMOS 'Daily AQI by County' de esta concatenaci√≥n\n",
    "    # ya que no tiene Latitude, Longitude, Datum\n",
    "    for key, df in dataframes_por_codigo.items():\n",
    "        if key != 'Daily AQI by County': # EXCLUSI√ìN AQUI\n",
    "            # Asegurarse de que cada DF tenga las columnas necesarias. Si no las tiene, a√±adir NaN.\n",
    "            for col in expected_location_cols:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = pd.NA\n",
    "            # Seleccionamos solo las columnas relevantes antes de concatenar\n",
    "            dfs_to_concat.append(df[expected_location_cols])\n",
    "        else:\n",
    "            print(f\"INFO: Excluyendo '{key}' de la concatenaci√≥n para 'location' ya que no contiene todas las columnas de ubicaci√≥n (Latitude, Longitude, Datum).\")\n",
    "\n",
    "\n",
    "    # Concatenar DataFrames relevantes (ahora sin Daily AQI by County)\n",
    "    if not dfs_to_concat:\n",
    "        print(\"ADVERTENCIA: No hay DataFrames con datos de Latitud/Longitud para generar la tabla 'location'.\")\n",
    "        return # Salir de la funci√≥n si no hay DFs para concatenar\n",
    "\n",
    "    df_completo = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "\n",
    "    # Filtrar y limpiar\n",
    "    # dropna en las columnas que definen una ubicaci√≥n √∫nica y son obligatorias\n",
    "    ubicaciones = (\n",
    "        df_completo\n",
    "        .dropna(subset=['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum'])\n",
    "        .drop_duplicates(subset=['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum'])\n",
    "        .reset_index(drop=True)\n",
    "        .copy()\n",
    "    )\n",
    "    ubicaciones['location_id'] = ubicaciones.index + 1\n",
    "\n",
    "    # Reordenar las columnas para que 'location_id' est√© al principio\n",
    "    ubicaciones = ubicaciones[['location_id'] + expected_location_cols]\n",
    "\n",
    "\n",
    "    # --- Depuraci√≥n adicional para verificar las columnas y datos ---\n",
    "    print(f\"\\n--- DEPURATION: DataFrame 'ubicaciones' antes de exportar ---\")\n",
    "    print(f\"Columnas finales del DataFrame 'ubicaciones': {ubicaciones.columns.tolist()}\")\n",
    "    print(f\"Primeras 10 filas del DataFrame 'ubicaciones':\\n{ubicaciones.head(10)}\")\n",
    "    print(f\"N√∫mero de filas en 'ubicaciones': {len(ubicaciones)}\")\n",
    "    print(f\"Tipos de datos de las columnas en 'ubicaciones':\\n{ubicaciones.dtypes}\")\n",
    "    \n",
    "    # Contar nulos para columnas clave\n",
    "    print(f\"Conteo de nulos en columnas clave de 'ubicaciones':\\n{ubicaciones[['State Code', 'County Code', 'Latitude', 'Longitude', 'Datum']].isnull().sum()}\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "    # -------------------------------------------------------------\n",
    "\n",
    "    # Funci√≥n para limpiar texto y manejar valores NaN para SQL\n",
    "    def limpiar(valor):\n",
    "        if pd.isna(valor):\n",
    "            return 'NULL'\n",
    "        return str(valor).strip().replace(\"'\", \"''\")\n",
    "\n",
    "    # Exportar archivo SQL\n",
    "    with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"INSERT INTO location (location_id, state_name, state_code, county_name, county_code, latitude, longitude, datum) VALUES\\n\")\n",
    "        \n",
    "        for i, fila in ubicaciones.iterrows():\n",
    "            valores_finales = [\n",
    "                fila['location_id'],\n",
    "                f\"'{limpiar(fila['State Name'])}'\" if limpiar(fila['State Name']) != 'NULL' else 'NULL',\n",
    "                f\"{int(fila['State Code'])}\" if pd.notna(fila['State Code']) else 'NULL',\n",
    "                f\"'{limpiar(fila['County Name'])}'\" if limpiar(fila['County Name']) != 'NULL' else 'NULL',\n",
    "                f\"{int(fila['County Code'])}\" if pd.notna(fila['County Code']) else 'NULL',\n",
    "                f\"{float(fila['Latitude'])}\",\n",
    "                f\"{float(fila['Longitude'])}\",\n",
    "                f\"'{limpiar(fila['Datum'])}'\" if limpiar(fila['Datum']) != 'NULL' else 'NULL'\n",
    "            ]\n",
    "\n",
    "            linea = f\"({', '.join(map(str, valores_finales))})\"\n",
    "            f.write(linea + (\",\\n\" if i < len(ubicaciones) - 1 else \";\\n\"))\n",
    "\n",
    "    print(f\"‚úÖ Archivo '{nombre_archivo}' generado con {len(ubicaciones)} ubicaciones completas.\")\n",
    "\n",
    "# --- 4. Llamada a la funci√≥n para generar el archivo SQL ---\n",
    "exportar_sql_completo(dataframes_por_codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4d7f568-99cf-4487-94d1-2e7131b1b457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando datos para dim_aqi_categoria...\n",
      "Primeras 5 filas de dim_aqi_categoria_df:\n",
      "                 nombre_categoria  min_aqi_rango  max_aqi_rango color_asociado\n",
      "0                            Good              0             50          Green\n",
      "1                        Moderate             51            100         Yellow\n",
      "2  Unhealthy for Sensitive Groups            101            150         Orange\n",
      "3                       Unhealthy            151            200            Red\n",
      "4                  Very Unhealthy            201            300         Purple\n",
      "N√∫mero total de categor√≠as AQI a cargar: 6\n",
      "‚úÖ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_dim_aqi_categoria.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta de salida ---\n",
    "# Aseg√∫rate de que esta ruta sea donde quieres que se guarde el archivo SQL\n",
    "output_sql_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Ajusta si es necesario\n",
    "\n",
    "# --- 2. Definir los datos de las categor√≠as AQI ---\n",
    "aqi_categorias_data = [\n",
    "    {\"nombre_categoria\": \"Good\", \"min_aqi_rango\": 0, \"max_aqi_rango\": 50, \"color_asociado\": \"Green\"},\n",
    "    {\"nombre_categoria\": \"Moderate\", \"min_aqi_rango\": 51, \"max_aqi_rango\": 100, \"color_asociado\": \"Yellow\"},\n",
    "    {\"nombre_categoria\": \"Unhealthy for Sensitive Groups\", \"min_aqi_rango\": 101, \"max_aqi_rango\": 150, \"color_asociado\": \"Orange\"},\n",
    "    {\"nombre_categoria\": \"Unhealthy\", \"min_aqi_rango\": 151, \"max_aqi_rango\": 200, \"color_asociado\": \"Red\"},\n",
    "    {\"nombre_categoria\": \"Very Unhealthy\", \"min_aqi_rango\": 201, \"max_aqi_rango\": 300, \"color_asociado\": \"Purple\"},\n",
    "    {\"nombre_categoria\": \"Hazardous\", \"min_aqi_rango\": 301, \"max_aqi_rango\": 500, \"color_asociado\": \"Maroon\"}\n",
    "]\n",
    "\n",
    "# Crear un DataFrame de Pandas\n",
    "df_aqi_categorias = pd.DataFrame(aqi_categorias_data)\n",
    "\n",
    "# No necesitamos sk_aqi_categoria ya que es AUTO_INCREMENT en la BBDD\n",
    "\n",
    "print(\"Generando datos para dim_aqi_categoria...\")\n",
    "print(f\"Primeras 5 filas de dim_aqi_categoria_df:\\n{df_aqi_categorias.head()}\")\n",
    "print(f\"N√∫mero total de categor√≠as AQI a cargar: {len(df_aqi_categorias)}\")\n",
    "\n",
    "# --- 3. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_dim_aqi_categoria.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO dim_aqi_categoria (nombre_categoria, min_aqi_rango, max_aqi_rango, color_asociado) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_aqi_categorias.iterrows():\n",
    "        # Escapar comillas simples en los nombres si fuera necesario (aunque aqu√≠ no lo son)\n",
    "        nombre_categoria = fila['nombre_categoria'].replace(\"'\", \"''\")\n",
    "        color_asociado = fila['color_asociado'].replace(\"'\", \"''\")\n",
    "\n",
    "        linea = (\n",
    "            f\"('{nombre_categoria}', \"\n",
    "            f\"{fila['min_aqi_rango']}, \"\n",
    "            f\"{fila['max_aqi_rango']}, \"\n",
    "            f\"'{color_asociado}')\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_categorias) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f213982-6675-4c0a-a0f6-799fc5a445a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo datos de par√°metros de los DataFrames...\n",
      "‚úÖ Cargado y extra√≠do par√°metros de: CO_42101\n",
      "‚úÖ Cargado y extra√≠do par√°metros de: NO2_42602\n",
      "‚úÖ Cargado y extra√≠do par√°metros de: SO2_42401\n",
      "‚úÖ Cargado y extra√≠do par√°metros de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_unique_parameters' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_unique_parameters': ['Parameter Code', 'Parameter Name']\n",
      "Primeras 5 filas del DataFrame 'df_unique_parameters':\n",
      "   Parameter Code          Parameter Name\n",
      "0           42101         Carbon monoxide\n",
      "1           42602  Nitrogen dioxide (NO2)\n",
      "2           42401          Sulfur dioxide\n",
      "3           44201                   Ozone\n",
      "N√∫mero total de par√°metros √∫nicos a cargar: 4\n",
      "Tipos de datos de las columnas en 'df_unique_parameters':\n",
      "Parameter Code     int64\n",
      "Parameter Name    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_parameter.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto (donde est√°n tus carpetas de datos) ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path # Guardar el SQL en la misma carpeta base\n",
    "\n",
    "# --- 3. Cargar los DataFrames de contaminantes ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_parameters = []\n",
    "\n",
    "print(\"Extrayendo datos de par√°metros de los DataFrames...\")\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=['Parameter Code', 'Parameter Name'])\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Seleccionar solo las columnas de inter√©s y a√±adir a la lista\n",
    "        all_parameters.append(df[['Parameter Code', 'Parameter Name']])\n",
    "        print(f\"‚úÖ Cargado y extra√≠do par√°metros de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {key} en {path}. Ser√° omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_parameters:\n",
    "    print(\"No se pudieron cargar datos de par√°metros de ning√∫n archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar todos los DataFrames de par√°metros\n",
    "df_parameters = pd.concat(all_parameters, ignore_index=True)\n",
    "\n",
    "# Obtener par√°metros √∫nicos\n",
    "df_unique_parameters = df_parameters.drop_duplicates(subset=['Parameter Code']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_unique_parameters' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_unique_parameters': {df_unique_parameters.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_unique_parameters':\\n{df_unique_parameters.head()}\")\n",
    "print(f\"N√∫mero total de par√°metros √∫nicos a cargar: {len(df_unique_parameters)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_unique_parameters':\\n{df_unique_parameters.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_parameter.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO parameter (parameter_code, parameter_name) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_unique_parameters.iterrows():\n",
    "        # Limpiar y escapar nombres para SQL\n",
    "        parameter_name = str(fila['Parameter Name']).strip().replace(\"'\", \"''\") if pd.notna(fila['Parameter Name']) else 'NULL'\n",
    "        parameter_code = int(fila['Parameter Code']) if pd.notna(fila['Parameter Code']) else 'NULL'\n",
    "\n",
    "        # Asegurarse de que 'NULL' no tenga comillas\n",
    "        name_for_sql = f\"'{parameter_name}'\" if parameter_name != 'NULL' else 'NULL'\n",
    "        code_for_sql = str(parameter_code) if parameter_code != 'NULL' else 'NULL'\n",
    "\n",
    "        linea = f\"({code_for_sql}, {name_for_sql})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_unique_parameters) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4909a74b-fbac-4317-84d7-6993bd97b3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrayendo datos de m√©todos de los DataFrames...\n",
      "‚úÖ Cargado y extra√≠do m√©todos de: CO_42101\n",
      "‚úÖ Cargado y extra√≠do m√©todos de: NO2_42602\n",
      "‚úÖ Cargado y extra√≠do m√©todos de: SO2_42401\n",
      "‚úÖ Cargado y extra√≠do m√©todos de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_unique_methods' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_unique_methods': ['Method Code', 'Method Name', 'Method Type']\n",
      "Primeras 5 filas del DataFrame 'df_unique_methods':\n",
      "   Method Code                                        Method Name Method Type\n",
      "0          554  INSTRUMENTAL - Gas Filter Correlation Thermo E...         FRM\n",
      "1           54              INSTRUMENTAL - NONDISPERSIVE INFRARED         FRM\n",
      "2          593  INSTRUMENTAL - Gas Filter Correlation Teledyne...         FRM\n",
      "3           93  INSTRUMENTAL - GAS FILTER CORRELATION CO ANALYZER         FRM\n",
      "4          588  INSTRUMENTAL - Gas Filter Correlation Ecotech ...         FRM\n",
      "N√∫mero total de m√©todos √∫nicos a cargar: 45\n",
      "Tipos de datos de las columnas en 'df_unique_methods':\n",
      "Method Code     int64\n",
      "Method Name    object\n",
      "Method Type    object\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_method.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto (donde est√°n tus carpetas de datos) ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path # Guardar el SQL en la misma carpeta base\n",
    "\n",
    "# --- 3. Cargar los DataFrames de contaminantes para extraer los m√©todos ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_methods = []\n",
    "\n",
    "print(\"Extrayendo datos de m√©todos de los DataFrames...\")\n",
    "\n",
    "# Columnas esperadas en los CSV para la tabla method\n",
    "expected_method_cols = ['Method Code', 'Method Name', 'Method Type']\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=expected_method_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # A√±adir a la lista\n",
    "        all_methods.append(df[expected_method_cols])\n",
    "        print(f\"‚úÖ Cargado y extra√≠do m√©todos de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {key} en {path}. Ser√° omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_methods:\n",
    "    print(\"No se pudieron cargar datos de m√©todos de ning√∫n archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar todos los DataFrames de m√©todos\n",
    "df_methods = pd.concat(all_methods, ignore_index=True)\n",
    "\n",
    "# Obtener m√©todos √∫nicos\n",
    "# Usamos todas las columnas del m√©todo para definir la unicidad\n",
    "df_unique_methods = df_methods.drop_duplicates(subset=['Method Code', 'Method Name', 'Method Type']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_unique_methods' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_unique_methods': {df_unique_methods.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_unique_methods':\\n{df_unique_methods.head()}\")\n",
    "print(f\"N√∫mero total de m√©todos √∫nicos a cargar: {len(df_unique_methods)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_unique_methods':\\n{df_unique_methods.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_method.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO method (method_code, method_name, method_type) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_unique_methods.iterrows():\n",
    "        # Funci√≥n para limpiar y manejar NULLs para SQL\n",
    "        def clean_for_sql(value, is_numeric=False):\n",
    "            if pd.isna(value):\n",
    "                return 'NULL'\n",
    "            if is_numeric:\n",
    "                return str(int(value)) if value == value else 'NULL' # Manejar NaN para enteros\n",
    "            return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "\n",
    "        method_code = clean_for_sql(fila['Method Code'], is_numeric=True)\n",
    "        method_name = clean_for_sql(fila['Method Name'])\n",
    "        method_type = clean_for_sql(fila['Method Type'])\n",
    "\n",
    "        linea = f\"({method_code}, {method_name}, {method_type})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_unique_methods) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b34e2d9-3bf9-47b8-a3c2-7466f1e8da03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexi√≥n a la base de datos...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contrase√±a de MySQL:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Motor de conexi√≥n a 'calidad_aire' creado exitosamente.\n",
      "Extrayendo datos de estaciones (site_num, state_code, county_code) de los DataFrames horarios...\n",
      "‚úÖ Cargado y extra√≠do estaciones de: CO_42101\n",
      "‚úÖ Cargado y extra√≠do estaciones de: NO2_42602\n",
      "‚úÖ Cargado y extra√≠do estaciones de: SO2_42401\n",
      "‚úÖ Cargado y extra√≠do estaciones de: Ozone_44201\n",
      "\n",
      "--- DEPURATION: Estaciones √∫nicas del CSV antes de unir con location ---\n",
      "Primeras 5 filas:\n",
      "   State Code  County Code  Site Num\n",
      "0           1           73        23\n",
      "1           1           73      1003\n",
      "2           1           73      2059\n",
      "3           1           73      6004\n",
      "4           2           20        18\n",
      "N√∫mero total de estaciones √∫nicas en CSV: 1590\n",
      "--------------------------------------------------\n",
      "\n",
      "Obteniendo 'location_id' de la tabla 'location' de MySQL...\n",
      "‚úÖ Cargadas 1590 ubicaciones de la tabla 'location'.\n",
      "Primeras 5 filas de 'df_db_locations':\n",
      "   location_id  state_code  county_code\n",
      "0            1           1           73\n",
      "1            2           1           73\n",
      "2            3           1           73\n",
      "3            4           1           73\n",
      "4            5           2           20\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_final_stations' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_final_stations': ['site_num', 'location_id']\n",
      "Primeras 5 filas del DataFrame 'df_final_stations':\n",
      "   site_num  location_id\n",
      "0        23            1\n",
      "1        23            2\n",
      "2        23            3\n",
      "3        23            4\n",
      "4        23          809\n",
      "N√∫mero total de estaciones (site_num con location_id) a cargar: 6752\n",
      "Tipos de datos de las columnas en 'df_final_stations':\n",
      "site_num       int64\n",
      "location_id    int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_station.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass # Importar getpass\n",
    "\n",
    "# --- 1. Configuraci√≥n de la conexi√≥n a la base de datos (USANDO TU M√âTODO) ---\n",
    "print(\"Configurando conexi√≥n a la base de datos...\")\n",
    "# Solicitar credenciales de forma segura\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \") # getpass para ocultar la entrada\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\" # Tu base de datos es 'calidad_aire'\n",
    "\n",
    "# Crear motor de conexi√≥n con SQLAlchemy\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "    print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 3. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 4. Cargar los DataFrames de contaminantes para extraer Site Num ---\n",
    "dataframes_paths = {\n",
    "    'CO_42101': os.path.join(base_project_path, 'CO_42101', 'hourly_42101_2015', 'hourly_42101_2015.csv'),\n",
    "    'NO2_42602': os.path.join(base_project_path, 'NO2_42602', 'hourly_42602_2015', 'hourly_42602_2015.csv'),\n",
    "    'SO2_42401': os.path.join(base_project_path, 'SO2_42401', 'hourly_42401_2015', 'hourly_42401_2015.csv'),\n",
    "    'Ozone_44201': os.path.join(base_project_path, 'Ozone_44201', 'hourly_44201_2015', 'hourly_44201_2015.csv'),\n",
    "}\n",
    "\n",
    "all_station_data = []\n",
    "\n",
    "print(\"Extrayendo datos de estaciones (site_num, state_code, county_code) de los DataFrames horarios...\")\n",
    "\n",
    "expected_station_cols = ['State Code', 'County Code', 'Site Num'] # Columnas necesarias del CSV\n",
    "\n",
    "for key, path in dataframes_paths.items():\n",
    "    try:\n",
    "        # Cargamos solo las columnas necesarias para mayor eficiencia\n",
    "        df = pd.read_csv(path, usecols=expected_station_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        all_station_data.append(df[expected_station_cols])\n",
    "        print(f\"‚úÖ Cargado y extra√≠do estaciones de: {key}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {key} en {path}. Ser√° omitido.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en {key}. Verifica los nombres de las columnas en el CSV.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar {key}: {e}\")\n",
    "\n",
    "if not all_station_data:\n",
    "    print(\"No se pudieron cargar datos de estaciones de ning√∫n archivo. Saliendo.\")\n",
    "    exit()\n",
    "\n",
    "df_raw_stations = pd.concat(all_station_data, ignore_index=True)\n",
    "\n",
    "# Obtener combinaciones √∫nicas de State Code, County Code, Site Num\n",
    "df_unique_stations_csv = df_raw_stations.drop_duplicates(subset=['State Code', 'County Code', 'Site Num']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: Estaciones √∫nicas del CSV antes de unir con location ---\")\n",
    "print(f\"Primeras 5 filas:\\n{df_unique_stations_csv.head()}\")\n",
    "print(f\"N√∫mero total de estaciones √∫nicas en CSV: {len(df_unique_stations_csv)}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 5. Obtener location_id de la base de datos usando el motor SQLAlchemy ---\n",
    "print(\"Obteniendo 'location_id' de la tabla 'location' de MySQL...\")\n",
    "try:\n",
    "    # Leer la tabla 'location' de MySQL\n",
    "    df_db_locations = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", engine)\n",
    "    \n",
    "    # Asegurarse de que los tipos de datos coincidan para el merge\n",
    "    df_db_locations['state_code'] = df_db_locations['state_code'].astype(int)\n",
    "    df_db_locations['county_code'] = df_db_locations['county_code'].astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ Cargadas {len(df_db_locations)} ubicaciones de la tabla 'location'.\")\n",
    "    print(f\"Primeras 5 filas de 'df_db_locations':\\n{df_db_locations.head()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al obtener 'location_id' de la base de datos: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. Unir los datos de estaciones del CSV con los location_id de la BBDD ---\n",
    "# Cambiar nombres de columnas en df_unique_stations_csv para el merge\n",
    "df_unique_stations_csv = df_unique_stations_csv.rename(columns={\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code'\n",
    "})\n",
    "\n",
    "df_final_stations = pd.merge(\n",
    "    df_unique_stations_csv,\n",
    "    df_db_locations,\n",
    "    on=['state_code', 'county_code'],\n",
    "    how='inner' # Solo queremos estaciones para las que tenemos un location_id v√°lido\n",
    ")\n",
    "\n",
    "# Seleccionar y renombrar columnas para que coincidan con la tabla 'station'\n",
    "df_final_stations = df_final_stations[['Site Num', 'location_id']].rename(columns={'Site Num': 'site_num'})\n",
    "\n",
    "# Eliminar duplicados finales en caso de que m√∫ltiples combinaciones state/county/site num apunten a la misma station (no deber√≠a pasar con inner join)\n",
    "df_final_stations = df_final_stations.drop_duplicates(subset=['site_num', 'location_id']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_final_stations' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_final_stations': {df_final_stations.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_final_stations':\\n{df_final_stations.head()}\")\n",
    "print(f\"N√∫mero total de estaciones (site_num con location_id) a cargar: {len(df_final_stations)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_final_stations':\\n{df_final_stations.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 7. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_station.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO station (site_num, location_id) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_final_stations.iterrows():\n",
    "        site_num_val = int(fila['site_num']) if pd.notna(fila['site_num']) else 'NULL'\n",
    "        location_id_val = int(fila['location_id']) if pd.notna(fila['location_id']) else 'NULL'\n",
    "\n",
    "        linea = f\"({site_num_val}, {location_id_val})\"\n",
    "        f.write(linea + (\",\\n\" if i < len(df_final_stations) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c17288-d5f8-4750-bfb1-c4f30d547902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexi√≥n a la base de datos (solo para referencia, no se usa para leer aqu√≠)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- 1. Configuraci√≥n de la conexi√≥n a la base de datos (USANDO TU M√âTODO) ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Aunque no necesitamos la conexi√≥n para GENERAR el SQL, la mantenemos para consistencia\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# y porque es parte de tu flujo de trabajo. No se usar√° para leer de la BBDD en este script.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigurando conexi√≥n a la base de datos (solo para referencia, no se usa para leer aqu√≠)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m usuario \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce tu usuario de MySQL (por defecto \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m contrase√±a \u001b[38;5;241m=\u001b[39m getpass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce tu contrase√±a de MySQL: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m host \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntroduce el host (por defecto \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unicornenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\unicornenv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass\n",
    "\n",
    "# --- 1. Configuraci√≥n de la conexi√≥n a la base de datos (USANDO TU M√âTODO) ---\n",
    "# Aunque no necesitamos la conexi√≥n para GENERAR el SQL, la mantenemos para consistencia\n",
    "# y porque es parte de tu flujo de trabajo. No se usar√° para leer de la BBDD en este script.\n",
    "print(\"Configurando conexi√≥n a la base de datos (solo para referencia, no se usa para leer aqu√≠)...\")\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "\n",
    "try:\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "    print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "    # No salimos aqu√≠ porque la conexi√≥n no es estrictamente necesaria para generar el SQL\n",
    "    # pero el usuario debe saber si falla.\n",
    "\n",
    "# --- 2. Definir la ruta base de tu proyecto y del archivo CBSA ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Aseg√∫rate que esta ruta sea correcta!\n",
    "cbsa_data_path = os.path.join(base_project_path, 'Daily AQI by CBSA', 'daily_aqi_by_cbsa_2015', 'daily_aqi_by_cbsa_2015.csv')\n",
    "\n",
    "# --- 3. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 4. Cargar el DataFrame de aqi_cbsa ---\n",
    "print(f\"\\nCargando datos para aqi_cbsa desde: {cbsa_data_path}...\")\n",
    "\n",
    "# Columnas esperadas en el CSV para la tabla aqi_cbsa\n",
    "expected_cbsa_cols = [\n",
    "    'CBSA Code', 'CBSA', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "\n",
    "try:\n",
    "    df_aqi_cbsa = pd.read_csv(cbsa_data_path, usecols=expected_cbsa_cols)\n",
    "    df_aqi_cbsa.columns = df_aqi_cbsa.columns.str.strip() # Limpiar nombres de columna\n",
    "    print(f\"‚úÖ Archivo '{os.path.basename(cbsa_data_path)}' cargado exitosamente.\")\n",
    "    print(f\"Total de filas le√≠das: {len(df_aqi_cbsa)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Archivo no encontrado para aqi_cbsa en {cbsa_data_path}. Verifica la ruta y que el archivo exista.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo aqi_cbsa. Verifica los nombres de las columnas en el CSV.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR inesperado al procesar aqi_cbsa: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Renombrar columnas para que coincidan con la tabla de MySQL\n",
    "df_aqi_cbsa = df_aqi_cbsa.rename(columns={\n",
    "    'CBSA Code': 'cbsa_code',\n",
    "    'CBSA': 'cbsa_name',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "})\n",
    "\n",
    "# Convertir la columna 'date' a formato de fecha\n",
    "df_aqi_cbsa['date'] = pd.to_datetime(df_aqi_cbsa['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Manejar valores NaN/None para inserci√≥n SQL (convertir a cadena 'NULL')\n",
    "def handle_null_for_sql(value, is_string=True):\n",
    "    if pd.isna(value):\n",
    "        return 'NULL'\n",
    "    if is_string:\n",
    "        return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "    return str(value) # Para n√∫meros, no comillas\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_aqi_cbsa' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_aqi_cbsa': {df_aqi_cbsa.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_aqi_cbsa':\\n{df_aqi_cbsa.head()}\")\n",
    "print(f\"N√∫mero total de registros a cargar en aqi_cbsa: {len(df_aqi_cbsa)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_aqi_cbsa':\\n{df_aqi_cbsa.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 5. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_aqi_cbsa.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO aqi_cbsa (cbsa_code, cbsa_name, date, aqi, category, defining_parameter, defining_site, sites_reporting) VALUES\\n\")\n",
    "    \n",
    "    for i, fila in df_aqi_cbsa.iterrows():\n",
    "        cbsa_code_val = handle_null_for_sql(fila['cbsa_code'], is_string=False)\n",
    "        cbsa_name_val = handle_null_for_sql(fila['cbsa_name'])\n",
    "        date_val = handle_null_for_sql(fila['date'])\n",
    "        aqi_val = handle_null_for_sql(fila['aqi'], is_string=False)\n",
    "        category_val = handle_null_for_sql(fila['category'])\n",
    "        defining_parameter_val = handle_null_for_sql(fila['defining_parameter'])\n",
    "        defining_site_val = handle_null_for_sql(fila['defining_site'])\n",
    "        sites_reporting_val = handle_null_for_sql(fila['sites_reporting'], is_string=False)\n",
    "\n",
    "        linea = (\n",
    "            f\"({cbsa_code_val}, {cbsa_name_val}, {date_val}, {aqi_val}, \"\n",
    "            f\"{category_val}, {defining_parameter_val}, {defining_site_val}, {sites_reporting_val})\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_cbsa) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d868c000-21ff-4d99-87b4-480d99548cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando datos para aqi_county desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv...\n",
      "‚úÖ Archivo 'daily_aqi_by_county_2015.csv' cargado exitosamente.\n",
      "Total de filas le√≠das: 320535\n",
      "\n",
      "--- DEPURATION: DataFrame 'df_aqi_county' antes de exportar ---\n",
      "Columnas finales del DataFrame 'df_aqi_county': ['state_code', 'county_code', 'date', 'aqi', 'category', 'defining_parameter', 'defining_site', 'sites_reporting']\n",
      "Primeras 5 filas del DataFrame 'df_aqi_county':\n",
      "   state_code  county_code        date  aqi  category defining_parameter  \\\n",
      "0           1            3  2015-01-03   38      Good              PM2.5   \n",
      "1           1            3  2015-01-06   55  Moderate              PM2.5   \n",
      "2           1            3  2015-01-09   60  Moderate              PM2.5   \n",
      "3           1            3  2015-01-12   52  Moderate              PM2.5   \n",
      "4           1            3  2015-01-15   35      Good              PM2.5   \n",
      "\n",
      "  defining_site  sites_reporting  \n",
      "0   01-003-0010                1  \n",
      "1   01-003-0010                1  \n",
      "2   01-003-0010                1  \n",
      "3   01-003-0010                1  \n",
      "4   01-003-0010                1  \n",
      "N√∫mero total de registros a cargar en aqi_county: 320535\n",
      "Tipos de datos de las columnas en 'df_aqi_county':\n",
      "state_code             int64\n",
      "county_code            int64\n",
      "date                  object\n",
      "aqi                    int64\n",
      "category              object\n",
      "defining_parameter    object\n",
      "defining_site         object\n",
      "sites_reporting        int64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "\n",
      "‚úÖ Archivo 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\insert_aqi_county.sql' generado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# No necesitamos importar create_engine ni getpass de nuevo si el motor ya est√° definido\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto y del archivo County ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "county_data_path = os.path.join(base_project_path, 'Daily AQI by County', 'daily_aqi_by_county_2015', 'daily_aqi_by_county_2015.csv')\n",
    "\n",
    "# --- 2. Definir la ruta de salida para el archivo SQL ---\n",
    "output_sql_path = base_project_path\n",
    "\n",
    "# --- 3. Cargar el DataFrame de aqi_county ---\n",
    "print(f\"\\nCargando datos para aqi_county desde: {county_data_path}...\")\n",
    "\n",
    "# Columnas esperadas en el CSV para la tabla aqi_county\n",
    "expected_county_cols = [\n",
    "    'State Code', 'County Code', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "\n",
    "try:\n",
    "    df_aqi_county = pd.read_csv(county_data_path, usecols=expected_county_cols)\n",
    "    df_aqi_county.columns = df_aqi_county.columns.str.strip() # Limpiar nombres de columna\n",
    "    print(f\"‚úÖ Archivo '{os.path.basename(county_data_path)}' cargado exitosamente.\")\n",
    "    print(f\"Total de filas le√≠das: {len(df_aqi_county)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Archivo no encontrado para aqi_county en {county_data_path}. Verifica la ruta y que el archivo exista.\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo aqi_county. Verifica los nombres de las columnas en el CSV.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR inesperado al procesar aqi_county: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Renombrar columnas para que coincidan con la tabla de MySQL\n",
    "df_aqi_county = df_aqi_county.rename(columns={\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "}) #\n",
    "\n",
    "# Convertir la columna 'date' a formato de fecha\n",
    "df_aqi_county['date'] = pd.to_datetime(df_aqi_county['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Manejar valores NaN/None para inserci√≥n SQL (convertir a cadena 'NULL')\n",
    "def handle_null_for_sql(value, is_string=True):\n",
    "    if pd.isna(value):\n",
    "        return 'NULL'\n",
    "    if is_string:\n",
    "        return f\"'{str(value).strip().replace(\"'\", \"''\")}'\"\n",
    "    return str(value) # Para n√∫meros, no comillas\n",
    "\n",
    "print(f\"\\n--- DEPURATION: DataFrame 'df_aqi_county' antes de exportar ---\")\n",
    "print(f\"Columnas finales del DataFrame 'df_aqi_county': {df_aqi_county.columns.tolist()}\")\n",
    "print(f\"Primeras 5 filas del DataFrame 'df_aqi_county':\\n{df_aqi_county.head()}\")\n",
    "print(f\"N√∫mero total de registros a cargar en aqi_county: {len(df_aqi_county)}\")\n",
    "print(f\"Tipos de datos de las columnas en 'df_aqi_county':\\n{df_aqi_county.dtypes}\")\n",
    "print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "# --- 4. Generar el script SQL de inserci√≥n ---\n",
    "nombre_archivo_sql = os.path.join(output_sql_path, \"insert_aqi_county.sql\")\n",
    "\n",
    "with open(nombre_archivo_sql, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"INSERT INTO aqi_county (state_code, county_code, date, aqi, category, defining_parameter, defining_site, sites_reporting) VALUES\\n\") #\n",
    "    \n",
    "    for i, fila in df_aqi_county.iterrows():\n",
    "        state_code_val = handle_null_for_sql(fila['state_code'], is_string=False)\n",
    "        county_code_val = handle_null_for_sql(fila['county_code'], is_string=False)\n",
    "        date_val = handle_null_for_sql(fila['date'])\n",
    "        aqi_val = handle_null_for_sql(fila['aqi'], is_string=False)\n",
    "        category_val = handle_null_for_sql(fila['category'])\n",
    "        defining_parameter_val = handle_null_for_sql(fila['defining_parameter'])\n",
    "        defining_site_val = handle_null_for_sql(fila['defining_site'])\n",
    "        sites_reporting_val = handle_null_for_sql(fila['sites_reporting'], is_string=False)\n",
    "\n",
    "        linea = (\n",
    "            f\"({state_code_val}, {county_code_val}, {date_val}, {aqi_val}, \"\n",
    "            f\"{category_val}, {defining_parameter_val}, {defining_site_val}, {sites_reporting_val})\"\n",
    "        )\n",
    "        f.write(linea + (\",\\n\" if i < len(df_aqi_county) - 1 else \";\\n\"))\n",
    "\n",
    "print(f\"‚úÖ Archivo '{nombre_archivo_sql}' generado exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2662cccd-5503-4a5b-ae45-1a1e0d11ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO CARGA MASIVA DE AQI_CBSA ---\n",
      "‚ùå ERROR al TRUNCATE la tabla 'aqi_cbsa': module 'pandas' has no attribute 'text'\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2015\\daily_aqi_by_cbsa_2015.csv...\n",
      "‚úÖ A√±o 2015 procesado y cargado. Filas cargadas: 169851. Tiempo: 5.44 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2016\\daily_aqi_by_cbsa_2016.csv...\n",
      "‚úÖ A√±o 2016 procesado y cargado. Filas cargadas: 170424. Tiempo: 7.36 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2017\\daily_aqi_by_cbsa_2017.csv...\n",
      "‚úÖ A√±o 2017 procesado y cargado. Filas cargadas: 173601. Tiempo: 9.31 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2018\\daily_aqi_by_cbsa_2018.csv...\n",
      "‚úÖ A√±o 2018 procesado y cargado. Filas cargadas: 172699. Tiempo: 7.82 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2019\\daily_aqi_by_cbsa_2019.csv...\n",
      "‚úÖ A√±o 2019 procesado y cargado. Filas cargadas: 170430. Tiempo: 8.20 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2020\\daily_aqi_by_cbsa_2020.csv...\n",
      "‚úÖ A√±o 2020 procesado y cargado. Filas cargadas: 169779. Tiempo: 8.47 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2021\\daily_aqi_by_cbsa_2021.csv...\n",
      "‚úÖ A√±o 2021 procesado y cargado. Filas cargadas: 170364. Tiempo: 9.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2022\\daily_aqi_by_cbsa_2022.csv...\n",
      "‚úÖ A√±o 2022 procesado y cargado. Filas cargadas: 168627. Tiempo: 7.30 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2023\\daily_aqi_by_cbsa_2023.csv...\n",
      "‚úÖ A√±o 2023 procesado y cargado. Filas cargadas: 169558. Tiempo: 8.94 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_cbsa del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by CBSA\\daily_aqi_by_cbsa_2024\\daily_aqi_by_cbsa_2024.csv...\n",
      "‚úÖ A√±o 2024 procesado y cargado. Filas cargadas: 109683. Tiempo: 5.69 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE AQI_CBSA COMPLETADA ---\n",
      "Total de filas cargadas en 'aqi_cbsa': 1645016\n",
      "Tiempo total de ejecuci√≥n: 77.59 segundos.\n",
      "‚ùå ERROR al verificar el conteo de filas en 'aqi_cbsa': module 'pandas' has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from getpass import getpass\n",
    "import time # Para medir el tiempo de ejecuci√≥n\n",
    "\n",
    "# --- 0. Configuraci√≥n de la conexi√≥n a la base de datos (si el motor no est√° activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o est√°s ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente secci√≥n para crear el 'engine'.\n",
    "# Si el 'engine' de un paso anterior sigue activo, deja esta secci√≥n COMENTADA.\n",
    "\n",
    "# print(\"Configurando conexi√≥n a la base de datos...\")\n",
    "# usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "# contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "# host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "# bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "# try:\n",
    "#     engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "#     print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "#     print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "#     exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los a√±os a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024 (el rango es exclusivo en el final)\n",
    "\n",
    "# --- 3. Columnas esperadas en los CSV y sus mapeos a la tabla MySQL ---\n",
    "expected_cbsa_cols = [\n",
    "    'CBSA Code', 'CBSA', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "]\n",
    "mysql_col_names = {\n",
    "    'CBSA Code': 'cbsa_code',\n",
    "    'CBSA': 'cbsa_name',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "}\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla antes de la carga masiva (opcional, pero recomendado si es la primera carga completa) ---\n",
    "# Si est√°s ejecutando esto por primera vez para todos los a√±os, TRUNCATE es necesario.\n",
    "# Si est√°s a√±adiendo a√±os nuevos a una tabla ya existente con a√±os anteriores, NO uses TRUNCATE.\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE AQI_CBSA ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(pd.text(\"TRUNCATE TABLE aqi_cbsa;\"))\n",
    "        connection.commit()\n",
    "    print(\"‚úÖ Tabla 'aqi_cbsa' TRUNCATED (vac√≠a) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al TRUNCATE la tabla 'aqi_cbsa': {e}\")\n",
    "    # Considera si quieres salir o continuar si el TRUNCATE falla.\n",
    "    # Por ahora, seguiremos si el error no es cr√≠tico.\n",
    "\n",
    "# --- 5. Procesar y cargar datos a√±o por a√±o ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "for year in years:\n",
    "    cbsa_data_dir = os.path.join(base_project_path, 'Daily AQI by CBSA', f'daily_aqi_by_cbsa_{year}')\n",
    "    cbsa_data_file = os.path.join(cbsa_data_dir, f'daily_aqi_by_cbsa_{year}.csv')\n",
    "\n",
    "    print(f\"\\nProcesando y cargando datos para aqi_cbsa del a√±o {year} desde: {cbsa_data_file}...\")\n",
    "    start_time_year = time.time()\n",
    "\n",
    "    try:\n",
    "        # Cargar el CSV\n",
    "        df = pd.read_csv(cbsa_data_file, usecols=expected_cbsa_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Renombrar columnas\n",
    "        df = df.rename(columns=mysql_col_names)\n",
    "        \n",
    "        # Convertir la columna 'date' a formato de fecha\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Opcional: convertir columnas a tipos de datos espec√≠ficos antes de to_sql\n",
    "        # Esto puede ayudar con la inferencia de tipos de SQLAlchemy\n",
    "        df['cbsa_code'] = df['cbsa_code'].astype('Int64') # Usar Int64 para manejar NaN como NULL\n",
    "        df['aqi'] = df['aqi'].astype('Int64')\n",
    "        df['sites_reporting'] = df['sites_reporting'].astype('Int64')\n",
    "\n",
    "        rows_in_chunk = len(df)\n",
    "        total_rows_loaded += rows_in_chunk\n",
    "\n",
    "        # Usar to_sql para insertar en la base de datos\n",
    "        # if_exists='append' para a√±adir al final de la tabla\n",
    "        # index=False para no insertar el √≠ndice del DataFrame como columna\n",
    "        # chunksize para insertar en lotes (importante para grandes DataFrames)\n",
    "        df.to_sql('aqi_cbsa', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "\n",
    "        end_time_year = time.time()\n",
    "        print(f\"‚úÖ A√±o {year} procesado y cargado. Filas cargadas: {rows_in_chunk}. Tiempo: {end_time_year - start_time_year:.2f} segundos.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {year} en {cbsa_data_file}. Se omitir√° este a√±o.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo de {year}. Verifica los nombres de las columnas en el CSV. Se omitir√° este a√±o.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar el a√±o {year}: {e}. Se omitir√° este a√±o.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE AQI_CBSA COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'aqi_cbsa': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecuci√≥n: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(pd.text(\"SELECT COUNT(*) FROM aqi_cbsa;\")).scalar()\n",
    "    print(f\"‚úÖ Conteo final de filas en la tabla 'aqi_cbsa' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al verificar el conteo de filas en 'aqi_cbsa': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5828f41c-f899-44cd-8b91-f8f5b51e5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- INICIANDO CARGA MASIVA DE AQI_COUNTY ---\n",
      "‚úÖ Tabla 'aqi_county' TRUNCATED (vac√≠a) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2015\\daily_aqi_by_county_2015.csv...\n",
      "‚úÖ A√±o 2015 procesado y cargado. Filas cargadas: 320535. Tiempo: 14.26 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2016\\daily_aqi_by_county_2016.csv...\n",
      "‚úÖ A√±o 2016 procesado y cargado. Filas cargadas: 321071. Tiempo: 15.09 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2017\\daily_aqi_by_county_2017.csv...\n",
      "‚úÖ A√±o 2017 procesado y cargado. Filas cargadas: 326801. Tiempo: 15.53 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2018\\daily_aqi_by_county_2018.csv...\n",
      "‚úÖ A√±o 2018 procesado y cargado. Filas cargadas: 327543. Tiempo: 15.34 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2019\\daily_aqi_by_county_2019.csv...\n",
      "‚úÖ A√±o 2019 procesado y cargado. Filas cargadas: 326046. Tiempo: 16.17 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2020\\daily_aqi_by_county_2020.csv...\n",
      "‚úÖ A√±o 2020 procesado y cargado. Filas cargadas: 325138. Tiempo: 15.10 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2021\\daily_aqi_by_county_2021.csv...\n",
      "‚úÖ A√±o 2021 procesado y cargado. Filas cargadas: 326540. Tiempo: 15.55 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2022\\daily_aqi_by_county_2022.csv...\n",
      "‚úÖ A√±o 2022 procesado y cargado. Filas cargadas: 324422. Tiempo: 14.70 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2023\\daily_aqi_by_county_2023.csv...\n",
      "‚úÖ A√±o 2023 procesado y cargado. Filas cargadas: 325393. Tiempo: 15.61 segundos.\n",
      "\n",
      "Procesando y cargando datos para aqi_county del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Daily AQI by County\\daily_aqi_by_county_2024\\daily_aqi_by_county_2024.csv...\n",
      "‚úÖ A√±o 2024 procesado y cargado. Filas cargadas: 206919. Tiempo: 9.97 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE AQI_COUNTY COMPLETADA ---\n",
      "Total de filas cargadas en 'aqi_county': 3130408\n",
      "Tiempo total de ejecuci√≥n: 147.33 segundos.\n",
      "‚úÖ Conteo final de filas en la tabla 'aqi_county' en MySQL: 3130408\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text # Importar 'text' de sqlalchemy\n",
    "import time\n",
    "\n",
    "# --- 0. Configuraci√≥n de la conexi√≥n a la base de datos (si el motor no est√° activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o est√°s ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente secci√≥n para crear el 'engine'.\n",
    "# Si el 'engine' de un paso anterior sigue activo, deja esta secci√≥n COMENTADA.\n",
    "\n",
    "# print(\"Configurando conexi√≥n a la base de base de datos...\")\n",
    "# usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "# contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "# host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "# bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "# try:\n",
    "#     engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "#     print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "#     print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "#     exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # ¬°Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los a√±os a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "\n",
    "# --- 3. Columnas esperadas en los CSV y sus mapeos a la tabla MySQL ---\n",
    "expected_county_cols = [\n",
    "    'State Code', 'County Code', 'Date', 'AQI', 'Category',\n",
    "    'Defining Parameter', 'Defining Site', 'Number of Sites Reporting'\n",
    "] #\n",
    "mysql_col_names_county = {\n",
    "    'State Code': 'state_code',\n",
    "    'County Code': 'county_code',\n",
    "    'Date': 'date',\n",
    "    'AQI': 'aqi',\n",
    "    'Category': 'category',\n",
    "    'Defining Parameter': 'defining_parameter',\n",
    "    'Defining Site': 'defining_site',\n",
    "    'Number of Sites Reporting': 'sites_reporting'\n",
    "} #\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla antes de la carga masiva (opcional, pero recomendado si es la primera carga completa) ---\n",
    "# Si la tabla ya estaba completamente vac√≠a y sabes que esta es la primera carga, puedes ejecutarlo.\n",
    "# Si no, considera omitirlo o usar un DELETE WHERE para un rango de fechas.\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE AQI_COUNTY ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE aqi_county;\")) # Usar text() de sqlalchemy\n",
    "        connection.commit()\n",
    "    print(\"‚úÖ Tabla 'aqi_county' TRUNCATED (vac√≠a) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al TRUNCATE la tabla 'aqi_county': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Aseg√∫rate de que la tabla est√© vac√≠a o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos a√±o por a√±o ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "for year in years:\n",
    "    county_data_dir = os.path.join(base_project_path, 'Daily AQI by County', f'daily_aqi_by_county_{year}')\n",
    "    county_data_file = os.path.join(county_data_dir, f'daily_aqi_by_county_{year}.csv')\n",
    "\n",
    "    print(f\"\\nProcesando y cargando datos para aqi_county del a√±o {year} desde: {county_data_file}...\")\n",
    "    start_time_year = time.time()\n",
    "\n",
    "    try:\n",
    "        # Cargar el CSV\n",
    "        df = pd.read_csv(county_data_file, usecols=expected_county_cols)\n",
    "        df.columns = df.columns.str.strip() # Limpiar nombres de columna\n",
    "        \n",
    "        # Renombrar columnas\n",
    "        df = df.rename(columns=mysql_col_names_county)\n",
    "        \n",
    "        # Convertir la columna 'date' a formato de fecha\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Opcional: convertir columnas a tipos de datos espec√≠ficos antes de to_sql\n",
    "        df['state_code'] = df['state_code'].astype('Int64')\n",
    "        df['county_code'] = df['county_code'].astype('Int64')\n",
    "        df['aqi'] = df['aqi'].astype('Int64')\n",
    "        df['sites_reporting'] = df['sites_reporting'].astype('Int64')\n",
    "\n",
    "        rows_in_chunk = len(df)\n",
    "        total_rows_loaded += rows_in_chunk\n",
    "\n",
    "        # Usar to_sql para insertar en la base de datos\n",
    "        df.to_sql('aqi_county', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "\n",
    "        end_time_year = time.time()\n",
    "        print(f\"‚úÖ A√±o {year} procesado y cargado. Filas cargadas: {rows_in_chunk}. Tiempo: {end_time_year - start_time_year:.2f} segundos.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {year} en {county_data_file}. Se omitir√° este a√±o.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo de {year}. Verifica los nombres de las columnas en el CSV. Se omitir√° este a√±o.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar el a√±o {year}: {e}. Se omitir√° este a√±o.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE AQI_COUNTY COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'aqi_county': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecuci√≥n: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM aqi_county;\")).scalar() # Usar text() de sqlalchemy\n",
    "    print(f\"‚úÖ Conteo final de filas en la tabla 'aqi_county' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al verificar el conteo de filas en 'aqi_county': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d779f9db-9057-457a-a34e-c1465f6d2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexi√≥n a la base de datos...\n",
      "‚úÖ Motor de conexi√≥n ya existente y activo.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "‚úÖ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\n",
      "‚úÖ Tabla 'measurement' TRUNCATED (vac√≠a) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2015: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "‚ùå ADVERTENCIA: Archivo no encontrado para CO en 2016 (C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv). Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2016\\hourly_42602_2016.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2016\\hourly_42401_2016.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2016\\hourly_44201_2016.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2016: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2017\\hourly_42101_2017.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2017\\hourly_42602_2017.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2017\\hourly_42401_2017.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2017\\hourly_44201_2017.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2017: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2018\\hourly_42101_2018.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2018\\hourly_42602_2018.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2018\\hourly_42401_2018.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2018\\hourly_44201_2018.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2018: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2019\\hourly_42101_2019.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2019\\hourly_42602_2019.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2019\\hourly_42401_2019.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2019\\hourly_44201_2019.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2019: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2020\\hourly_42101_2020.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2020\\hourly_42602_2020.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2020\\hourly_42401_2020.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2020\\hourly_44201_2020.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2020: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2021\\hourly_42101_2021.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2021\\hourly_42602_2021.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2021\\hourly_42401_2021.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2021\\hourly_44201_2021.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2021: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2022\\hourly_42101_2022.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2022\\hourly_42602_2022.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2022\\hourly_42401_2022.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2022\\hourly_44201_2022.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2022: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2023\\hourly_42101_2023.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2023\\hourly_42602_2023.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2023\\hourly_42401_2023.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2023\\hourly_44201_2023.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2023: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2024\\hourly_42101_2024.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2024\\hourly_42602_2024.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de NO2 para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2024\\hourly_42401_2024.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de SO2 para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2024\\hourly_44201_2024.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de Ozone para 2024: Usecols do not match columns, columns expected but not found: ['Arithmetic Mean']. Se omitir√° este archivo.\n",
      "\n",
      "--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\n",
      "Total de filas cargadas en 'measurement': 0\n",
      "Tiempo total de ejecuci√≥n: 0.21 segundos.\n",
      "‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuraci√≥n de la conexi√≥n a la base de datos (si el motor no est√° activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o est√°s ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente secci√≥n para crear el 'engine'.\n",
    "\n",
    "print(\"Configurando conexi√≥n a la base de datos...\")\n",
    "# Asegurarse de que el motor exista, si no, lo crea.\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None: # Comprueba si 'engine' no est√° definido o es None\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "        print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Motor de conexi√≥n ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los a√±os y contaminantes a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "pollutants = {\n",
    "    'CO': '42101',\n",
    "    'NO2': '42602',\n",
    "    'SO2': '42401',\n",
    "    'Ozone': '44201'\n",
    "} # Nombres de carpeta y c√≥digos de par√°metro\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    # Unir station con location para tener state_code y county_code en df_station\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"‚úÖ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla measurement (opcional, pero recomendado si es la primera carga completa) ---\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE measurement;\"))\n",
    "        connection.commit()\n",
    "    print(\"‚úÖ Tabla 'measurement' TRUNCATED (vac√≠a) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al TRUNCATE la tabla 'measurement': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Aseg√∫rate de que la tabla est√© vac√≠a o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos a√±o por a√±o, contaminante por contaminante ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "# Columnas esperadas en los CSV horarios y sus mapeos a la tabla MySQL\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Arithmetic Mean', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Arithmetic Mean': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "} #\n",
    "\n",
    "for year in years:\n",
    "    for pollutant_name, pollutant_code in pollutants.items():\n",
    "        # *** CORRECCI√ìN DE LA RUTA DEL ARCHIVO ***\n",
    "        data_file_path = os.path.join(\n",
    "            base_project_path,\n",
    "            f'{pollutant_name}_{pollutant_code}',  # Carpeta principal del contaminante\n",
    "            f'hourly_{pollutant_code}_{year}',     # Subcarpeta del a√±o\n",
    "            f'hourly_{pollutant_code}_{year}.csv'  # Nombre del archivo CSV\n",
    "        ) #\n",
    "\n",
    "        print(f\"\\nProcesando y cargando datos para '{pollutant_name}' del a√±o {year} desde: {data_file_path}...\")\n",
    "        start_time_file = time.time()\n",
    "\n",
    "        if not os.path.exists(data_file_path):\n",
    "            print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {pollutant_name} en {year} ({data_file_path}). Se omitir√° este archivo.\")\n",
    "            continue\n",
    "\n",
    "        rows_processed_file = 0\n",
    "        \n",
    "        # Usamos chunksize para leer el CSV en partes\n",
    "        try:\n",
    "            for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000): # Leer en chunks de 50,000 filas\n",
    "                chunk.columns = chunk.columns.str.strip() # Limpiar nombres de columna\n",
    "                \n",
    "                # Renombrar columnas para la tabla measurement\n",
    "                chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "                \n",
    "                # Renombrar columnas para los joins\n",
    "                chunk = chunk.rename(columns={\n",
    "                    'State Code': 'state_code',\n",
    "                    'County Code': 'county_code',\n",
    "                    'Site Num': 'site_num',\n",
    "                    'Parameter Code': 'parameter_code',\n",
    "                    'Method Code': 'method_code'\n",
    "                })\n",
    "\n",
    "                # Convertir fechas a formato YYYY-MM-DD\n",
    "                chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                # Realizar los merges para obtener los IDs de las claves for√°neas\n",
    "                # Merge con station (que ya tiene location_id, state_code, county_code)\n",
    "                # Usamos how='inner' para solo mantener los registros que tienen una estaci√≥n v√°lida\n",
    "                chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                        on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "                \n",
    "                # Merge con parameter\n",
    "                chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "                \n",
    "                # Merge con method\n",
    "                chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "                # Seleccionar solo las columnas necesarias para la inserci√≥n en 'measurement'\n",
    "                final_chunk = chunk_merged[[\n",
    "                    'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                    'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "                ]]\n",
    "\n",
    "                # Convertir a tipos que acepten NaN (Int64 para enteros, Float64 para floats)\n",
    "                final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "                final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "                final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "                # 'uncertainty' y 'qualifier' son VARCHAR, se manejar√°n como objetos/strings\n",
    "                \n",
    "                # Insertar el chunk procesado en la base de datos\n",
    "                final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "                \n",
    "                rows_processed_file += len(final_chunk)\n",
    "                total_rows_loaded += len(final_chunk)\n",
    "\n",
    "            end_time_file = time.time()\n",
    "            print(f\"‚úÖ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"‚ùó ADVERTENCIA: El archivo {data_file_path} est√° vac√≠o. Se omitir√°.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo de {pollutant_name} para {year}. Verifica los nombres de las columnas en el CSV. Se omitir√° este archivo.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR inesperado al procesar el archivo de {pollutant_name} para {year}: {e}. Se omitir√° este archivo.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecuci√≥n: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e13ba5a-052e-476b-815d-39798a06e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexi√≥n a la base de datos...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contrase√±a de MySQL:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Motor de conexi√≥n a 'calidad_aire' creado exitosamente.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "‚úÖ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\n",
      "‚úÖ Tabla 'measurement' TRUNCATED (vac√≠a) exitosamente.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2015\\hourly_42101_2015.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2015.csv' procesado y cargado. Filas cargadas: 15463348. Tiempo: 1149.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2015\\hourly_42602_2015.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2015.csv' procesado y cargado. Filas cargadas: 19565543. Tiempo: 1163.33 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2015\\hourly_42401_2015.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2015.csv' procesado y cargado. Filas cargadas: 15036734. Tiempo: 3927.44 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2015 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2015\\hourly_44201_2015.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2015.csv' procesado y cargado. Filas cargadas: 38856199. Tiempo: 2241.09 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "‚ùå ADVERTENCIA: Archivo no encontrado para CO en 2016 (C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv). Se omitir√° este archivo.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2016\\hourly_42602_2016.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2016.csv' procesado y cargado. Filas cargadas: 19873458. Tiempo: 1170.05 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2016\\hourly_42401_2016.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2016.csv' procesado y cargado. Filas cargadas: 14893555. Tiempo: 849.91 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2016\\hourly_44201_2016.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2016.csv' procesado y cargado. Filas cargadas: 39634882. Tiempo: 2287.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2017\\hourly_42101_2017.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2017.csv' procesado y cargado. Filas cargadas: 14311759. Tiempo: 838.04 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2017\\hourly_42602_2017.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2017.csv' procesado y cargado. Filas cargadas: 19129388. Tiempo: 1107.67 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2017\\hourly_42401_2017.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2017.csv' procesado y cargado. Filas cargadas: 14110536. Tiempo: 810.59 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2017 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2017\\hourly_44201_2017.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2017.csv' procesado y cargado. Filas cargadas: 39079969. Tiempo: 2291.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2018\\hourly_42101_2018.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2018.csv' procesado y cargado. Filas cargadas: 13651003. Tiempo: 802.13 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2018\\hourly_42602_2018.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2018.csv' procesado y cargado. Filas cargadas: 18922705. Tiempo: 1087.13 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2018\\hourly_42401_2018.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2018.csv' procesado y cargado. Filas cargadas: 12917898. Tiempo: 756.15 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2018 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2018\\hourly_44201_2018.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2018.csv' procesado y cargado. Filas cargadas: 38571002. Tiempo: 2260.40 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2019\\hourly_42101_2019.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2019.csv' procesado y cargado. Filas cargadas: 12744642. Tiempo: 733.27 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2019\\hourly_42602_2019.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2019.csv' procesado y cargado. Filas cargadas: 18262436. Tiempo: 1063.00 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2019\\hourly_42401_2019.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2019.csv' procesado y cargado. Filas cargadas: 12620856. Tiempo: 740.31 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2019 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2019\\hourly_44201_2019.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2019.csv' procesado y cargado. Filas cargadas: 37042294. Tiempo: 2253.95 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2020\\hourly_42101_2020.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2020.csv' procesado y cargado. Filas cargadas: 12319915. Tiempo: 711.74 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2020\\hourly_42602_2020.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2020.csv' procesado y cargado. Filas cargadas: 18128317. Tiempo: 1055.69 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2020\\hourly_42401_2020.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2020.csv' procesado y cargado. Filas cargadas: 12308536. Tiempo: 740.17 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2020 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2020\\hourly_44201_2020.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2020.csv' procesado y cargado. Filas cargadas: 36742548. Tiempo: 2169.41 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2021\\hourly_42101_2021.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2021.csv' procesado y cargado. Filas cargadas: 11770539. Tiempo: 682.92 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2021\\hourly_42602_2021.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2021.csv' procesado y cargado. Filas cargadas: 17584267. Tiempo: 1074.87 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2021\\hourly_42401_2021.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2021.csv' procesado y cargado. Filas cargadas: 11688736. Tiempo: 699.32 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2021 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2021\\hourly_44201_2021.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2021.csv' procesado y cargado. Filas cargadas: 35760205. Tiempo: 2176.48 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2022\\hourly_42101_2022.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2022.csv' procesado y cargado. Filas cargadas: 10979660. Tiempo: 643.27 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2022\\hourly_42602_2022.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2022.csv' procesado y cargado. Filas cargadas: 17279558. Tiempo: 1019.80 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2022\\hourly_42401_2022.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2022.csv' procesado y cargado. Filas cargadas: 10892239. Tiempo: 641.74 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2022 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2022\\hourly_44201_2022.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2022.csv' procesado y cargado. Filas cargadas: 34722629. Tiempo: 2118.53 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2023\\hourly_42101_2023.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2023.csv' procesado y cargado. Filas cargadas: 10078678. Tiempo: 599.24 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2023\\hourly_42602_2023.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2023.csv' procesado y cargado. Filas cargadas: 16470734. Tiempo: 970.52 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2023\\hourly_42401_2023.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2023.csv' procesado y cargado. Filas cargadas: 10246326. Tiempo: 604.23 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2023 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2023\\hourly_44201_2023.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2023.csv' procesado y cargado. Filas cargadas: 34035669. Tiempo: 2045.06 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2024\\hourly_42101_2024.csv...\n",
      "‚úÖ Archivo 'hourly_42101_2024.csv' procesado y cargado. Filas cargadas: 5445891. Tiempo: 322.46 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'NO2' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\NO2_42602\\hourly_42602_2024\\hourly_42602_2024.csv...\n",
      "‚úÖ Archivo 'hourly_42602_2024.csv' procesado y cargado. Filas cargadas: 8831965. Tiempo: 522.05 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'SO2' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\SO2_42401\\hourly_42401_2024\\hourly_42401_2024.csv...\n",
      "‚úÖ Archivo 'hourly_42401_2024.csv' procesado y cargado. Filas cargadas: 5758121. Tiempo: 341.23 segundos.\n",
      "\n",
      "Procesando y cargando datos para 'Ozone' del a√±o 2024 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\Ozone_44201\\hourly_44201_2024\\hourly_44201_2024.csv...\n",
      "‚úÖ Archivo 'hourly_44201_2024.csv' procesado y cargado. Filas cargadas: 19376059. Tiempo: 1182.93 segundos.\n",
      "\n",
      "--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\n",
      "Total de filas cargadas en 'measurement': 755108799\n",
      "Tiempo total de ejecuci√≥n: 47853.20 segundos.\n",
      "‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: 755108799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuraci√≥n de la conexi√≥n a la base de datos (si el motor no est√° activo) ---\n",
    "# Si tu kernel de Jupyter fue reiniciado o est√°s ejecutando este script de forma independiente,\n",
    "# DESCOMENTA y ejecuta la siguiente secci√≥n para crear el 'engine'.\n",
    "\n",
    "print(\"Configurando conexi√≥n a la base de datos...\")\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None: # Comprueba si 'engine' no est√° definido o es None\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "        print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Motor de conexi√≥n ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir los a√±os y contaminantes a procesar ---\n",
    "years = range(2015, 2025) # De 2015 a 2024\n",
    "pollutants = {\n",
    "    'CO': '42101',\n",
    "    'NO2': '42602',\n",
    "    'SO2': '42401',\n",
    "    'Ozone': '44201'\n",
    "} # Nombres de carpeta y c√≥digos de par√°metro\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    # Unir station con location para tener state_code y county_code en df_station\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"‚úÖ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. TRUNCATE de la tabla measurement (opcional, pero recomendado si es la primera carga completa) ---\n",
    "print(\"\\n--- INICIANDO CARGA MASIVA DE MEASUREMENT ---\")\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        connection.execute(text(\"TRUNCATE TABLE measurement;\"))\n",
    "        connection.commit()\n",
    "    print(\"‚úÖ Tabla 'measurement' TRUNCATED (vac√≠a) exitosamente.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al TRUNCATE la tabla 'measurement': {e}\")\n",
    "    print(\"Continuando sin TRUNCATE. Aseg√∫rate de que la tabla est√© vac√≠a o no te importen los duplicados.\")\n",
    "\n",
    "# --- 5. Procesar y cargar datos a√±o por a√±o, contaminante por contaminante ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Sample Measurement', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Sample Measurement': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "}\n",
    "\n",
    "for year in years:\n",
    "    for pollutant_name, pollutant_code in pollutants.items():\n",
    "        data_file_path = os.path.join(\n",
    "            base_project_path,\n",
    "            f'{pollutant_name}_{pollutant_code}',\n",
    "            f'hourly_{pollutant_code}_{year}',\n",
    "            f'hourly_{pollutant_code}_{year}.csv'\n",
    "        )\n",
    "\n",
    "        print(f\"\\nProcesando y cargando datos para '{pollutant_name}' del a√±o {year} desde: {data_file_path}...\")\n",
    "        start_time_file = time.time()\n",
    "\n",
    "        if not os.path.exists(data_file_path):\n",
    "            print(f\"‚ùå ADVERTENCIA: Archivo no encontrado para {pollutant_name} en {year} ({data_file_path}). Se omitir√° este archivo.\")\n",
    "            continue\n",
    "\n",
    "        rows_processed_file = 0\n",
    "        \n",
    "        try:\n",
    "            for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000,\n",
    "                                     on_bad_lines='skip', low_memory=False):\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "\n",
    "                chunk.columns = chunk.columns.str.strip()\n",
    "                \n",
    "                chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "                \n",
    "                chunk = chunk.rename(columns={\n",
    "                    'State Code': 'state_code',\n",
    "                    'County Code': 'county_code',\n",
    "                    'Site Num': 'site_num',\n",
    "                    'Parameter Code': 'parameter_code',\n",
    "                    'Method Code': 'method_code'\n",
    "                })\n",
    "\n",
    "                chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "                chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "                chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                        on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "                \n",
    "                chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "                \n",
    "                chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "                # *** MODIFICACI√ìN AQU√ç: A√±adir .copy() para evitar SettingWithCopyWarning ***\n",
    "                final_chunk = chunk_merged[[\n",
    "                    'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                    'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "                ]].copy()\n",
    "\n",
    "                final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "                final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "                final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "                \n",
    "                final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "                \n",
    "                rows_processed_file += len(final_chunk)\n",
    "                total_rows_loaded += len(final_chunk)\n",
    "\n",
    "            end_time_file = time.time()\n",
    "            print(f\"‚úÖ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"‚ùó ADVERTENCIA: El archivo {data_file_path} est√° vac√≠o o no contiene datos v√°lidos. Se omitir√°.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo de {pollutant_name} para {year}. Esto no deber√≠a pasar con 'Sample Measurement'. Por favor, verifica el CSV. Se omitir√° este archivo.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR inesperado al procesar el archivo de {pollutant_name} para {year}: {e}. Se omitir√° este archivo.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA MASIVA DE MEASUREMENT COMPLETADA ---\")\n",
    "print(f\"Total de filas cargadas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecuci√≥n: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "629aaf30-604b-4742-9364-561998f6081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce tu usuario de MySQL (por defecto 'root'):  root\n",
      "Introduce tu contrase√±a de MySQL:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Introduce el host (por defecto 'localhost'):  localhost\n",
      "Introduce el nombre de la base de datos (por defecto 'calidad_aire'):  calidad_aire\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando conexi√≥n a la base de datos...\n",
      "‚úÖ Motor de conexi√≥n ya existente y activo.\n",
      "\n",
      "Cargando tablas de dimensiones (station, parameter, method) desde MySQL...\n",
      "‚úÖ Tablas de dimensiones cargadas exitosamente:\n",
      "   - station: 6752 filas\n",
      "   - parameter: 4 filas\n",
      "   - method: 45 filas\n",
      "\n",
      "--- INICIANDO CARGA DEL ARCHIVO FALTANTE ---\n",
      "‚ö†Ô∏è ADVERTENCIA: La tabla 'measurement' NO ser√° truncada. Los datos se a√±adir√°n al final.\n",
      "\n",
      "Procesando y cargando datos para 'CO' del a√±o 2016 desde: C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project\\CO_42101\\hourly_42101_2016\\hourly_42101_2016.csv...\n",
      "‚ùå ERROR inesperado al procesar el archivo de CO para 2016: Usecols do not match columns, columns expected but not found: ['Units of Measure', 'State Code', 'County Code', 'Qualifier', 'Method Code', 'Date Local', 'Date of Last Change', 'MDL', 'POC', 'Sample Measurement', 'Parameter Code', 'Uncertainty', 'Site Num'].\n",
      "\n",
      "--- CARGA DEL ARCHIVO FALTANTE COMPLETADA ---\n",
      "Total de filas a√±adidas en 'measurement': 0\n",
      "Tiempo total de ejecuci√≥n para este archivo: 0.00 segundos.\n",
      "‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: 755108799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from getpass import getpass\n",
    "import time\n",
    "\n",
    "# --- 0. Configuraci√≥n de la conexi√≥n a la base de datos (si el motor no est√° activo) ---\n",
    "usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "host = input(\"Introduce el host (por defecto 'localhost'): \")  or \"localhost\"\n",
    "bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "\n",
    "# Crea el motor de conexi√≥n\n",
    "engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Configurando conexi√≥n a la base de datos...\")\n",
    "try:\n",
    "    if 'engine' not in locals() or engine is None:\n",
    "        usuario = input(\"Introduce tu usuario de MySQL (por defecto 'root'): \") or \"root\"\n",
    "        contrase√±a = getpass(\"Introduce tu contrase√±a de MySQL: \")\n",
    "        host = input(\"Introduce el host (por defecto 'localhost'): \") or \"localhost\"\n",
    "        bd = input(\"Introduce el nombre de la base de datos (por defecto 'calidad_aire'): \") or \"calidad_aire\"\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://{usuario}:{contrase√±a}@{host}/{bd}\")\n",
    "        print(f\"‚úÖ Motor de conexi√≥n a '{bd}' creado exitosamente.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Motor de conexi√≥n ya existente y activo.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al crear el motor de conexi√≥n: {e}\")\n",
    "    print(\"Por favor, verifica tus credenciales y los par√°metros de conexi√≥n.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Definir la ruta base de tu proyecto ---\n",
    "base_project_path = 'C:/Users/mjcd1/Desktop/CURSOS/Python Unicorn/Proyecto Integrador Final Calidad del Aire USA/Air Quality Data Project' # Aseg√∫rate que esta ruta sea correcta!\n",
    "\n",
    "# --- 2. Definir el contaminante y a√±o ESPEC√çFICO a procesar ---\n",
    "# *** SOLO PARA CARGAR EL ARCHIVO FALTANTE DE CO 2016 ***\n",
    "specific_year = 2016\n",
    "specific_pollutant_name = 'CO'\n",
    "specific_pollutant_code = '42101' #\n",
    "\n",
    "# --- 3. Cargar DataFrames de dimensiones (station, parameter, method) desde MySQL ---\n",
    "print(\"\\nCargando tablas de dimensiones (station, parameter, method) desde MySQL...\")\n",
    "try:\n",
    "    df_station = pd.read_sql(\"SELECT station_id, site_num, location_id FROM station\", con=engine)\n",
    "    df_location = pd.read_sql(\"SELECT location_id, state_code, county_code FROM location\", con=engine)\n",
    "    df_station = pd.merge(df_station, df_location, on='location_id', how='inner')\n",
    "    df_parameter = pd.read_sql(\"SELECT parameter_id, parameter_code FROM parameter\", con=engine)\n",
    "    df_method = pd.read_sql(\"SELECT method_id, method_code FROM method\", con=engine)\n",
    "\n",
    "    print(\"‚úÖ Tablas de dimensiones cargadas exitosamente:\")\n",
    "    print(f\"   - station: {len(df_station)} filas\")\n",
    "    print(f\"   - parameter: {len(df_parameter)} filas\")\n",
    "    print(f\"   - method: {len(df_method)} filas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al cargar tablas de dimensiones: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. NO TRUNCAR la tabla measurement (ya est√° cargada casi por completo) ---\n",
    "print(\"\\n--- INICIANDO CARGA DEL ARCHIVO FALTANTE ---\")\n",
    "print(\"‚ö†Ô∏è ADVERTENCIA: La tabla 'measurement' NO ser√° truncada. Los datos se a√±adir√°n al final.\")\n",
    "\n",
    "\n",
    "# --- 5. Procesar y cargar el archivo espec√≠fico ---\n",
    "total_rows_loaded = 0\n",
    "start_time_total = time.time()\n",
    "\n",
    "expected_measurement_cols = [\n",
    "    'State Code', 'County Code', 'Site Num', 'Parameter Code', 'POC',\n",
    "    'Date Local', 'Sample Measurement', 'Units of Measure', 'MDL',\n",
    "    'Uncertainty', 'Qualifier', 'Method Code', 'Date of Last Change'\n",
    "]\n",
    "mysql_col_names_measurement = {\n",
    "    'POC': 'poc',\n",
    "    'Date Local': 'date_local',\n",
    "    'Sample Measurement': 'value',\n",
    "    'Units of Measure': 'unit',\n",
    "    'MDL': 'mdl',\n",
    "    'Uncertainty': 'uncertainty',\n",
    "    'Qualifier': 'qualifier',\n",
    "    'Date of Last Change': 'date_last_change'\n",
    "}\n",
    "\n",
    "# Construye la ruta para el archivo de CO 2016\n",
    "data_file_path = os.path.join(\n",
    "    base_project_path,\n",
    "    f'{specific_pollutant_name}_{specific_pollutant_code}',\n",
    "    f'hourly_{specific_pollutant_code}_{specific_year}',\n",
    "    f'hourly_{specific_pollutant_code}_{specific_year}.csv' # Esperamos el nombre corregido\n",
    ")\n",
    "\n",
    "print(f\"\\nProcesando y cargando datos para '{specific_pollutant_name}' del a√±o {specific_year} desde: {data_file_path}...\")\n",
    "start_time_file = time.time()\n",
    "\n",
    "if not os.path.exists(data_file_path):\n",
    "    print(f\"‚ùå ERROR: El archivo '{os.path.basename(data_file_path)}' a√∫n NO se encuentra en la ruta esperada. Por favor, aseg√∫rate de haberlo RENOMBRADO a '{os.path.basename(data_file_path)}' y colocado en: '{os.path.dirname(data_file_path)}'.\")\n",
    "else:\n",
    "    rows_processed_file = 0\n",
    "    try:\n",
    "        for chunk in pd.read_csv(data_file_path, usecols=expected_measurement_cols, chunksize=50000,\n",
    "                                 on_bad_lines='skip', low_memory=False):\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "            \n",
    "            chunk = chunk.rename(columns=mysql_col_names_measurement)\n",
    "            \n",
    "            chunk = chunk.rename(columns={\n",
    "                'State Code': 'state_code',\n",
    "                'County Code': 'county_code',\n",
    "                'Site Num': 'site_num',\n",
    "                'Parameter Code': 'parameter_code',\n",
    "                'Method Code': 'method_code'\n",
    "            })\n",
    "\n",
    "            chunk['date_local'] = pd.to_datetime(chunk['date_local'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "            chunk['date_last_change'] = pd.to_datetime(chunk['date_last_change'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            chunk_merged = pd.merge(chunk, df_station[['station_id', 'site_num', 'state_code', 'county_code']],\n",
    "                                    on=['site_num', 'state_code', 'county_code'], how='inner')\n",
    "            \n",
    "            chunk_merged = pd.merge(chunk_merged, df_parameter, on='parameter_code', how='inner')\n",
    "            \n",
    "            chunk_merged = pd.merge(chunk_merged, df_method, on='method_code', how='inner')\n",
    "\n",
    "            final_chunk = chunk_merged[[\n",
    "                'station_id', 'parameter_id', 'method_id', 'poc', 'date_local',\n",
    "                'value', 'unit', 'mdl', 'uncertainty', 'qualifier', 'date_last_change'\n",
    "            ]].copy()\n",
    "\n",
    "            final_chunk['poc'] = final_chunk['poc'].astype('Int64')\n",
    "            final_chunk['value'] = final_chunk['value'].astype('Float64')\n",
    "            final_chunk['mdl'] = final_chunk['mdl'].astype('Float64')\n",
    "            \n",
    "            final_chunk.to_sql('measurement', con=engine, if_exists='append', index=False, chunksize=10000)\n",
    "            \n",
    "            rows_processed_file += len(final_chunk)\n",
    "            total_rows_loaded += len(final_chunk)\n",
    "\n",
    "        end_time_file = time.time()\n",
    "        print(f\"‚úÖ Archivo '{os.path.basename(data_file_path)}' procesado y cargado. Filas cargadas: {rows_processed_file}. Tiempo: {end_time_file - start_time_file:.2f} segundos.\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"‚ùó ADVERTENCIA: El archivo {data_file_path} est√° vac√≠o o no contiene datos v√°lidos. Se omitir√°.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå ERROR: Columna '{e}' no encontrada en el archivo de {specific_pollutant_name} para {specific_year}. Por favor, verifica el CSV. Se omitir√° este archivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR inesperado al procesar el archivo de {specific_pollutant_name} para {specific_year}: {e}.\")\n",
    "\n",
    "end_time_total = time.time()\n",
    "print(f\"\\n--- CARGA DEL ARCHIVO FALTANTE COMPLETADA ---\")\n",
    "print(f\"Total de filas a√±adidas en 'measurement': {total_rows_loaded}\")\n",
    "print(f\"Tiempo total de ejecuci√≥n para este archivo: {end_time_total - start_time_total:.2f} segundos.\")\n",
    "\n",
    "# --- 6. Verificar el conteo de filas en la BBDD (opcional) ---\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        result = connection.execute(text(\"SELECT COUNT(*) FROM measurement;\")).scalar()\n",
    "    print(f\"‚úÖ Conteo final de filas en la tabla 'measurement' en MySQL: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al verificar el conteo de filas en 'measurement': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da3afb-39c7-4b56-b342-e4b0270db382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (unicornenv)",
   "language": "python",
   "name": "unicornenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
